{"/fibonacci-backend-cache/backend-cache-coding":{"title":"Backend Cache Coding","data":{"backend-코드-작업#Backend 코드 작업":"","fibonaccicachecontroller#FibonacciCacheController":"FibonacciCacheController\n// ...\r\n@RestController\r\npublic class FibonacciCacheController {\r\n    @Value(\"${fibonacci.language}\") // 1)\r\n    private String language;\r\n\r\n    @Value(\"${fibonacci.api-key}\")  // 1)\r\n    private String apiKey;\r\n\r\n    private final FibonacciResultCacheService fibonacciResultCacheService;\r\n    private final FibonacciCalculateRestClient fibonacciCalculateRestClient;\r\n    private final FibonacciTaskQueue fibonacciTaskQueue;\r\n\r\n    public FibonacciCacheController(\r\n        FibonacciResultCacheService fibonacciResultCacheService,\r\n        FibonacciCalculateRestClient fibonacciCalculateRestClient,\r\n        FibonacciTaskQueue fibonacciTaskQueue\r\n    ){\r\n        this.fibonacciResultCacheService = fibonacciResultCacheService;\r\n        this.fibonacciCalculateRestClient = fibonacciCalculateRestClient;\r\n        this.fibonacciTaskQueue = fibonacciTaskQueue;\r\n    }\r\n\r\n    @GetMapping(\"/fibonacci/{number}\")\r\n    public String getFibonacci(\r\n            @PathVariable(\"number\") int number,\r\n            @RequestParam(value = \"api-key\", required = false) String apiKey\r\n    ){\r\n        if(number > 10){\r\n            if(!this.apiKey.equals(apiKey)){\r\n                throw new ApiKeyNotExistException(\"10 이상의 수에 대한 피보나치 계산은 API KEY 가 필요합니다.\");\r\n            }\r\n        }\r\n\r\n        return fibonacciResultCacheService\r\n                .selectFibonacci(number)\r\n                .map(BigDecimal::toPlainString)\r\n                .orElseGet(() -> {\r\n                    if(number > 1000){\r\n                        final long size = fibonacciTaskQueue.offerTask(number);\r\n                        return switch (language){\r\n                            case \"ko\" -> \"fibonacci(\" + number + \") 계산을 예약합니다. 남은 작업수 = \" + size;\r\n                            case \"en\" -> \"fibonacci(\" + number + \") has been scheduled. Remain task = \" + size;\r\n                            default -> \"Unsupported Language\";\r\n                        };\r\n                    }\r\n\r\n                    return fibonacciCalculateRestClient\r\n                            .requestGetFibonacci(number)\r\n                            .map(result -> {\r\n                                fibonacciResultCacheService.putResult(number, result);\r\n                                return result.toPlainString();\r\n                            })\r\n                            .orElseThrow(() -> new IllegalStateException(\r\n                                    \"피보나치 연산에 문제가 발생했습니다.\"\r\n                            ));\r\n\r\n                });\r\n    }\r\n}\n1) ${fibonacci.language}, ${fibonacci.api-key}\n쿠버네티스 레벨에서 주입한 환경변수입니다. Pod 내에서는 ConfigMap 에 선언한 설정변수를 환경변수로 주입받고, Pod 내에서 Docker Container 로 동작하는 fibonacci-backend-cache 모듈 내에서는 application.yml 파일 내에서 아래와 같이 환경변수를 주입받고 있습니다.\nfibonacci:\r\n  language: ${APP_LANGUAGE:en} # 여기\r\n  api-key: ${API_KEY:hello-welcome} # 여기\r\nspring:\r\n  data:\r\n    redis:\r\n      host: redis-service.default.svc.cluster.local\r\n      port: 6379","fibonacciresultcacheservice#FibonacciResultCacheService":"// ...\r\n@Service\r\npublic class FibonacciResultCacheService {\r\n    private final StringRedisTemplate stringRedisTemplate;\r\n\r\n    public FibonacciResultCacheService(\r\n            StringRedisTemplate stringRedisTemplate\r\n    ){\r\n        this.stringRedisTemplate = stringRedisTemplate;\r\n    }\r\n\r\n    private final String HASH_KEY = \"fibonacci:result-set\";\r\n\r\n    public Optional<BigDecimal> selectFibonacci(int number){\r\n        return getResult(String.valueOf(number))\r\n                .map(str -> new BigDecimal(str));\r\n    }\r\n\r\n    public Optional<String> getResult(String key){\r\n        HashOperations<String, String, String> hashOperations =\r\n                stringRedisTemplate.opsForHash();\r\n\r\n        return Optional\r\n                .ofNullable(hashOperations.get(HASH_KEY, key));\r\n    }\r\n\r\n    public void putResult(int n, BigDecimal result){\r\n        stringRedisTemplate.opsForHash().put(HASH_KEY, String.valueOf(n), result.toPlainString());\r\n    }\r\n}","fibonaccicalculaterestclient#FibonacciCalculateRestClient":"// ...\r\n@Component\r\npublic class FibonacciCalculateRestClient {\r\n    private final RestClient fibonacciClient = RestClient.create();\r\n    public Optional<BigDecimal> requestGetFibonacci(int number) {\r\n        String result = fibonacciClient.get()\r\n                .uri(\"http://fibonacci-backend-web-service:8080/fibonacci?number=\"+number) // 1)\r\n                .retrieve()\r\n                .onStatus(HttpStatusCode::isError, (request, response) -> {\r\n                    throw new RuntimeException(\"invalid server response \"+ response.getStatusText());\r\n                })\r\n                .body(String.class);\r\n\r\n        return Optional.ofNullable(new BigDecimal(result));\r\n    }\r\n}\n1)\n같은 클러스터 내에 같은 namespace 에 존재하는 다른 Deployment 를 호출할 때는 그 Deployment 가 속한 Service 의 이름을 지정하는 것만으로 호출이 가능합니다.\nfibonacci-backend-web-service 는 fibonacci-backend-web-deploy 라는 디플로이먼트의 네트워킹을 위한 서비스입니다.\nfibonacci-backend-deploy는 fibonacci-backend-web 모듈을 Deployment 로 배포하는 리소스입니다. fibonacc-backend-web 모듈은 fibonacci 계산을 Top Down 방식으로 계산하는 /fibonacci API를 가지고 있고, fibonacci-backend-cache 에서 호출합니다.","probe-헬스체크#Probe (헬스체크)":"probe 용도의 Controller 입니다. Probe 관련된 개념들은 이 깃헙 문서 내의 별도의 문서에 따로 개념정리를 해둘 예정입니다.HealthCheckController\n// ...\r\n\r\n@RestController\r\n@RequestMapping(\"/probe\")\r\npublic class HealthCheckController {\r\n\r\n    private final Logger logger = LoggerFactory.getLogger(HealthCheckController.class);\r\n\r\n    @GetMapping(\"/startup\")\r\n    public String startupCheck(){\r\n        logger.info(\"[startup probe] >>> OK\");\r\n        return \"START UP OK\";\r\n    }\r\n\r\n    @GetMapping(\"/ready\")\r\n    public String readinessCheck(){\r\n        logger.info(\"[readiness probe] >>> OK\");\r\n        return \"READY OK\";\r\n    }\r\n\r\n    @GetMapping(\"/live\")\r\n    public String livenessCheck(){\r\n        logger.info(\"[liveness probe] >>> OK\");\r\n        return \"OK\";\r\n    }\r\n\r\n}"}},"/fibonacci-backend-cache/backend-cache-jib-build":{"title":"Backend Cache Jib Build","data":{"gradle-jib-빌드-정의-이미지-생성-푸시#Gradle Jib 빌드 정의, 이미지 생성, 푸시":"도커이미지를 빌드,푸시하는 방식에 대해 정리해봅니다. Gradle Jib 빌드 방식에 대해서 정리하지만, Dockerfile 로 빌드하는 방식에 대해서도 문서의 마지막에 추가로 정리해두겠습니다.","참고#참고":"github.com/jib/jib-gradle-plugin\ngithub.com/GoogleContainerTools/jib","javakotlin-애플리케이션의-도커이미지-빌드-방식#Java/Kotlin 애플리케이션의 도커이미지 빌드 방식":"Java/Kotlin 애플리케이션의 Gradle 빌드 시 두가지를 선택할 수 있습니다\nGradle Jib 플러그인을 사용하는 빌드 & 푸시\nDockerfile 정의, Shell Script 를 이용한 빌드 & 푸시\n두 방법 중에는 일반적으로 Gradle Jib 을 이용한 방식이 많이 사용되는 편입니다.","gradle-jib-으로-docker-이미지-빌드--푸시#Gradle Jib 으로 Docker 이미지 빌드 & 푸시":"","plugin-추가#plugin 추가":"plugins {\r\n    // ...\r\n\tid 'com.google.cloud.tools.jib' version '3.4.0'\r\n}\n코틀린 DSL의 build.gradle.kts 파일에서는 아래와 같이 작성합니다.\nplugins {\r\n  // ...\r\n  id(\"com.google.cloud.tools.jib\") version \"3.4.0\"\r\n  // ...\r\n}","빌드스크립트-작성#빌드스크립트 작성":"jib {\r\n\tfrom {\r\n\t\timage = \"amazoncorretto:17\"\r\n\t}\r\n\r\n\tto{\r\n\t\timage = \"chagchagchag/fibonacci-backend-cache\"\r\n\t\ttags = [\"0.0.1\", \"0.0.1.fibonacci-backend-cache.01\", \"latest\" ]\r\n\t}\r\n\r\n\tcontainer{\r\n\t\tcreationTime = \"USE_CURRENT_TIMESTAMP\"\r\n\t}\r\n}\n위의 빌드스크립트의 경우 코틀린 문법과 다른 부분이 없기에 코틀린 DSL 의 build.gradle.kts 파일을 작성시에도 위의 내용을 그대로 사용하면 됩니다.젠킨스나 Github CI에서 빌드하는 것이 아닌 경우 개발자의 PC에 따라 CPU 가 달라지는 것으로 인해 애플 M1 등 여러가지 빌드 옵션을 직접 추가하거나 이런 작업들이 필요한 경우가 있습니다.이런 경우 jib 내의 from 구문에 아래와 같은 내용을 작성해주시면 됩니다.\n// ...\r\n\r\njib {\r\n    from {\r\n        image = \"amazoncorretto:17\"\r\n\r\n        platforms {\r\n            platform{\r\n                architecture = \"arm64\"\r\n                os = \"linux\"\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n// ...\n회사에서 실제 개발 작 시 필요한 내용들을 정의할 때 세부적인 내용들이 많이 필요할 수 있는데 자세한 설명은 github.com/jib/jib-gradle-plugin 에서 확인할 수 있습니다.","빌드--push#빌드 & Push":"이렇게 작성한 build 스크립트는 gradle 명령어로 빌드할 수도 있고, intellij 에서 jib 태스크를 실행해서 도커이미지를 빌드 후 푸시할 수도 있습니다.참고로 개발 PC에서 빌드 실행 시 Docker Desktop 이 실행 중이어야 합니다.","intellij-내에서-jib-태스크-실행#intellij 내에서 jib 태스크 실행":"인텔리제이 내에서는 아래와 같이 jib 태스크 버튼을 눌러서 태스크를 실행해주시면 됩니다.","gradlew-명령어로-실행#gradlew 명령어로 실행":"gradlew 명령어로 실행하는 것은 아래와 같이 하면 됩니다.\n./gradlew fibonacci-backend-cache:jib","컨테이너-이미지-동작-확인#컨테이너 이미지 동작 확인":"도커 이미지 pull (만약 로컬에 이미지가 없다면)\n$ docker pull chagchagchag/fibonacci-backend-cache:0.0.1\n도커 이미지 구동\ndocker container run --rm -d -p 8080:8080 --name fibonacci-backend-cache-local chagchagchag/fibonacci-backend-cache:0.0.1\r\n\r\n동작 확인\n$ curl http://localhost:8080/probe/healthcheck\r\nOK\n종료\ndocker container stop fibonacci-backend-cache-local\r\n\r\nfibonacci-backend-cache-local","dockerfile-로-docker-이미지-빌드--푸시#Dockerfile 로 Docker 이미지 빌드 & 푸시":"","dockerfile-정의-빌드--푸시#Dockerfile 정의, 빌드 & 푸시":"Dockerfile_simple\nFROM amazoncorretto:17\r\nWORKDIR deploy\r\nCOPY build/libs/fibonacci_backend_cache-0.0.1.jar app.jar\r\nENTRYPOINT [\"java\", \"-jar\", \"app.jar\"]\n이미지 빌드\ndocker build -f Dockerfile_simple --tag chagchagchag/fibonacci-backend-cache-simple:0.0.1 .\n컨테이너 구동\n$ docker container run --name fibonacci-backend-cache-simple --rm -d -p 8080:8080 chagchagchag/fibonacci-backend-cache-simple:0.0.1\r\n\r\n\r\n## 확인\r\n$ docker container ls\r\nCONTAINER ID   IMAGE                                               COMMAND               CREATED         STATUS\r\nPORTS                    NAMES\r\ncf160b8e1ae2   chagchagchag/fibonacci-backend-cache-simple:0.0.1   \"java -jar app.jar\"   6 seconds ago   Up 4 seconds   0.0.0.0:8080->8080/tcp   fibonacci-backend-cache-simple\nAPI 확인\n$ curl http://localhost:8080/probe/healthcheck\r\nOK\n종료&삭제\n$ docker container stop fibonacci-backend-cache-simple\r\nfibonacci-backend-cache-simple","이미지-리포지터리#이미지 리포지터리":"예전의 레거시 개발 방식에서는 Docker 이미지라는 것이 필요가 없었습니다. 현재는 Docker Container 방식의 이미지를 구동하는 방식이 대중화되어 있습니다. 이때 이미지 리포지터리에 개발버전이 잘못 Push 될 수 있는 경우가 있습니다.이런 이유로 가급적이면 개발 용도의 이미지 리포지터리와 Deploy (배포) 전용 이미지 리포지터리를 따로 운영해서 Github Action 등에서만 사용하는 Deploy 전용 이미지 리포지터리를 사용하는 것을 권장드립니다.\n보통 이미지 리포지터리는 S3 방식으로 운영되는 것으로 알고 있습니다. 따라서 AWS에서 ECR 을 운영,개발 용도로 분리해서 사용한다고 하더라도 용량에 큰 부담도 없고 비용이 그렇게 크게 발생하지는 않을 듯 해보입니다."}},"/fibonacci-backend-cache/backend-cache-graceful-shutdown":{"title":"Backend Cache Graceful Shutdown","data":{"graceful-shutdown-처리#Graceful Shutdown 처리":"Graceful Shutdown 시 작업의 종료 통지, 리소스 회수 등을 수행할 때 처리해줄 작업들에 대한 실습입니다."}},"/fibonacci-backend-cache/backend-cache-k8s-yaml":{"title":"Backend Cache K8s Yaml","data":{"k8s-리소스-정의--로컬-k8s-확인#k8s 리소스 정의 & 로컬 k8s 확인":"시크릿 리소스를 정의하고 사용하는 부분에서는 api-key 를 시크릿에 정의해서 사용합니다. 사실 api-key 를 Secret 에서 정의해두고 단 하나의 api-key 만으로 검사를 한다는 것은 조금 이상해보일 수도 있고 언뜻 예제가 이해가 안갈 수 있습니다.하지만, fibonacci-backend-cache 에서 다양한 쿠버네티스 리소스를 모두 다뤄보려 하다보니 시크릿을 사용하는 기능을 추가해서 억지로 끼워맞췄다는 점을 이해해주시기 바랍니다.","들어가기-전에#들어가기 전에..":"이번 페이지는 ArgoCD나 이런 것들을 설치하지 않은 간단한 버전의 클러스터를 기준으로 리소스들을 kubectl 로 하나 하나 만들어가면서 테스트해가면서 기능들을 완성하는 과정을 설명하기 위한 것이 목적입니다.이번 backend 예제를 구동할 kind 클러스터 정의는 cluster/single-cluster.yml 에 있고, 쉘스크립트는 cluster/create-single-cluster.sh 입니다.","namespace-정의#namespace 정의":"예제 테스트를 위한 namespace를 정의합니다. namespace 는 fibonacci 입니다.\n$ kubectl create ns fibonacci\r\nnamespace/fibonacci created","secret-정의#Secret 정의":"먼저 api 키를 정의합니다.\nkubectl -n fibonacci create secret generic fibonacci-backend-cache-secret --from-literal=api-key=abcd-efgh-ijkl-1111\n이렇게 생성한 시크릿은 아래와 같이 확인 가능합니다.\nkubectl -n fibonacci get secret fibonacci-backend-cache-secret -o yaml\r\n...\r\n\r\n\r\napiVersion: v1\r\ndata:\r\n  api-key: YWJjZC1lZmdoLWlqa2wtMTExMQ==\r\nkind: Secret\r\nmetadata:\r\n  creationTimestamp: \"2024-01-26T04:13:56Z\"\r\n  name: fibonacci-backend-cache-secret\r\n  namespace: fibonacci\r\n  resourceVersion: \"6919\"\r\n  uid: 2f641720-ee50-4223-8120-639ac984524c\r\ntype: Opaque\n만약 이미 배포되어있는 secret 내의 속성값의 디코딩 된 실제 값을 알고 싶다면 아래와 같이 base64 decoding 을 해주면 됩니다.\n$ echo \"YWJjZC1lZmdoLWlqa2wtMTExMQ==\" | base64 -d\r\nabcd-efgh-ijkl-1111\n혹시 평문 문자열이 시크릿 안에 들어갔을 때 어떻게 변하는지 확인하고 싶다면 아래와 같이 -n 옵션을 주어서 개행문자를 제거한 문자열에 대해 base64 인코딩을 해줍니다.\n$ echo -n \"abcd-efgh-ijkl-1111\" | base64\r\nYWJjZC1lZmdoLWlqa2wtMTExMQ==\n-n 옵션이 없으면 개행문자가 추가된 상태로 출력됩니다.","configmap-정의#ConfigMap 정의":"이번에는 ConfigMap 을 정의해봅니다.\r\nfibonacci-cache-config.yml\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: fibonacci-cache-config\r\n  namespace: fibonacci\r\ndata:\r\n  language: \"ko\"\ndata.language\ndata.language 라는 속성에 대한 값으로 ko 라는 값을 지정했습니다.\n이렇게 지정한 configmap 을 클러스터 내에 리소스로 생성되게끔 kubectl 로 요청해봅니다.\n$ kubectl apply -f fibonacci-cache-config.yml\r\nconfigmap/fibonacci-cache-config created\n생성되었는지 확인\n$ kubectl -n fibonacci get configmap fibonacci-cache-config\r\nNAME                     DATA   AGE\r\nfibonacci-cache-config   1      153m\r\n\r\n$ kubectl -n fibonacci get configmap fibonacci-cache-config -o yaml\r\napiVersion: v1\r\ndata:\r\n  language: ko\r\nkind: ConfigMap\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/last-applied-configuration: |\r\n      {\"apiVersion\":\"v1\",\"data\":{\"language\":\"ko\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"name\":\"fibonacci-cache-config\",\"namespace\":\"fibonacci\"}}\r\n  creationTimestamp: \"2024-01-26T04:34:48Z\"\r\n  name: fibonacci-cache-config\r\n  namespace: fibonacci\r\n  resourceVersion: \"8711\"\r\n  uid: 98d6a65b-4eeb-4ca5-8f0c-c23ac35bb479","pv-pvc-정의#PV, PVC 정의":"pv, pvc 를 이용해서 로그를 파일로 작성하는 예제입니다\npv, pvc에 대해 궁금한 점들이 있다면 아래의 문서들을 참고해주세요.\nPV, PVC\nStorageClass","pvc#PVC":"local-storage 타입의 StorageClass 로 정의한 볼륨을 마운트하는 예제입니다.\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: cache-log-storage-claim\r\n  namespace: fibonacci\r\nspec:\r\n  # kubectl get sc 명령을 통해 나타나는 StorageClass 중 하나를 선택했다.\r\n  # 로컬에서는 kind 버전에 따라 standard 가 나올수도 있고 local-storage 가 나올수도 있다.\r\n  storageClassName: standard\r\n  accessModes:\r\n    - ReadWriteOnce\r\n  resources:\r\n    requests:\r\n      storage: 100Mi\nmetadata.name : 스토리지 클래스 명\nspec.storageClassName\n사용하는 클러스터에 따라 local-storage 의 종류가 다르게 나옵니다. 꼭 컴퓨터에서 kubectl get sc 명령을 통해서 어떤 storage class 명으로 등록되어있는지 조회해보고 등록되어 있는 스토리지 클래스명을 사용하시면 됩니다.\n예제용도이고 로컬에서 구동하는 예제이기에 약식으로  local-storage 기반으로 구성했습니다. 실무에서는 NFS 또는 EBS 를 사용하는 경우가 더 많습니다.","deployment-정의#Deployment 정의":"apiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: fibonacci-backend-cache-deploy\r\n  namespace: fibonacci\r\nspec:\r\n  replicas: 2\r\n  strategy:\r\n    type: RollingUpdate\r\n    rollingUpdate:\r\n      maxSurge: 1\r\n      maxUnavailable: 0\r\n  selector:\r\n    matchLabels:\r\n      app: fibonacci-backend-cache\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: fibonacci-backend-cache\r\n    spec:\r\n      terminationGracePeriodSeconds: 60\r\n      volumes:\r\n        - name: cache-volume\r\n          emptyDir:\r\n            medium: Memory\r\n        - name: log-volume\r\n          persistentVolumeClaim:\r\n            claimName: cache-log-storage-claim\r\n      containers:\r\n        - name: fibonacci\r\n          image: chagchagchag/fibonacci-backend-cache:0.0.1\r\n          imagePullPolicy: Always\r\n          volumeMounts:\r\n            - mountPath: /fibonacci/logs\r\n              name: log-volume\r\n            - mountPath: /fibonacci/cache\r\n              name: cache-volume\r\n          env:\r\n            - name: APP_LANGUAGE\r\n              valueFrom:\r\n                configMapKeyRef:\r\n                  name: fibonacci-cache-config\r\n                  key: language\r\n            - name: API_KEY\r\n              valueFrom:\r\n                secretKeyRef:\r\n                  name: fibonacci-cache-secret\r\n                  key: api-key\r\n          lifecycle:\r\n            preStop:\r\n              exec:\r\n                command: [\"/bin/sh\",\"-c\",\"sleep 10\"]\r\n          readinessProbe:\r\n            httpGet:\r\n              path: /probe/healthcheck\r\n              port: 8080\r\n            initialDelaySeconds: 15\r\n            periodSeconds: 1\r\n            successThreshold: 2\r\n            failureThreshold: 3\r\n          livenessProbe:\r\n            httpGet:\r\n              path: /probe/healthcheck\r\n              port: 8080\r\n            initialDelaySeconds: 15\r\n            periodSeconds: 1\r\n            failureThreshold: 7\nspec.volumes 내에 volume 을 하나 추가해줬습니다.\nvolume 의 이름(name)은 log-volume 이라고 정의해줬고, persistentVolumeClame.clameName 을 지정해서 사용하려는 PersistentVolumeClaim 의 이름을 명시했습니다.","service-정의#Service 정의":"지금까지 만든 애플리케이션은 Pod 를 ReplicaSet 단위로 구동하는 Deployment, 볼륨 용도의 PV,PVC, 환경변수를 보관하는 ConfigMap, API Key 를 저장하기 위한 Secret 이라는 리소스를 두루 사용했습니다.이렇게 만든 애플리케이션에서 Deployment를 클러스터 안에서 네트워킹을 통해 접근할 수 있는 개체로 만드는 방법은 Service 리소스를 정의하는 것입니다.Service 는 Nodeport, ClusterIP 등과 같은 것들이 있고, 자세한 내용은 이번 Github Page 에서 정리하기에는 내용이 다소 길기에 TODO - http://chagchagchag.github.io/docs-k8s-resources 에 정리해두기로 했습니다. 개념 설명은 http://chagchagchag.github.io/docs-k8s-resources 을 참고해주시기 바랍니다.\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: fibonacci-backend-cache-service\r\n  namespace: fibonacci\r\nspec:\r\n  selector:\r\n    app: fibonacci-backend-cache\r\n  ports:\r\n    - protocol: TCP\r\n      port: 8080\n간단한 ClusterIP 에 대한 정의입니다. 쿠버네티스는 서비스의 타입을 별도로 지정하지 않으면 디폴트 설정으로 ClusterIP 타입으로 정의됩니다. Cluster IP 라는 단어 그대로 Cluster 내에서의 IP 를 의미하며, 클러스터 내에서만 식별이 가능한 IP 입니다.\n위에서 작성한 Cluster IP 서비스의 이름은 fibonacci-backend-cache-service 입니다. 그리고 spec.selector.app 에 유입/유출 네트워크를 연결해줄 Deployment 를 연결해줍니다. 위의 예제에서는 Service 가 fibonacci-backend-cache 라는 이름의 Deployment 를 연결해주고 있고, 연결할 TCP 포트는 8080 포트임을 명시하고 있습니다.","ingress-정의#ingress 정의":"ingress 의 역할은 80 포트 또는 443 포트로 유입되는 트래픽을 Service 타입에 바인딩해주는 역할을 합니다. \n만약 클러스터를 외부 트래픽에 대해 80, 443 포트 외의 포트를 개방하고 싶다면 Ingress 대신 NodePort 를 사용하면 됩니다. NodePort 는 30000 ~ 32768 범위의 포트를 허용가능합니다.\napiVersion: networking.k8s.io/v1\r\nkind: Ingress\r\nmetadata:\r\n  name: fibonacci-ingress\r\n  namespace: fibonacci\r\nspec:\r\n  rules:\r\n    - http:\r\n        paths:\r\n          - pathType: Prefix\r\n            path: /fibonacci\r\n            backend:\r\n              service:\r\n                name: fibonacci-backend-cache-service\r\n                port:\r\n                  number: 8080","리소스-정의-yaml-파일-통합#리소스 정의 YAML 파일 통합":"kustomize 를 사용한다면 이런 작업은 필요가 없겠지만, 단순 yaml 파일 기반으로 작업한다면 위의 yaml 파일들을 모두 순차적으로 실행시켜줘야 합니다.이렇게 하면 조금 귀찮아지기에 여기에서는 위의 모든 yaml 리소스 정의를 하나의 파일로 합친 리소스 정의 yaml 을 작성한 부분을 아래에 남겨둡니다.\napiVersion: v1\r\nkind: Namespace\r\nmetadata:\r\n  name: fibonacci\r\n---\r\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: cache-log-storage-claim\r\n  namespace: fibonacci\r\nspec:\r\n  # kubectl get sc 명령을 통해 나타나는 StorageClass 중 하나를 선택했다.\r\n  # 로컬에서는 kind 버전에 따라 standard 가 나올수도 있고 local-storage 가 나올수도 있다.\r\n  storageClassName: standard\r\n  accessModes:\r\n    - ReadWriteOnce\r\n  resources:\r\n    requests:\r\n      storage: 100Mi\r\n---\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: fibonacci-backend-cache-deploy\r\n  namespace: fibonacci\r\nspec:\r\n  replicas: 2\r\n  strategy:\r\n    type: RollingUpdate\r\n    rollingUpdate:\r\n      maxSurge: 1\r\n      maxUnavailable: 0\r\n  selector:\r\n    matchLabels:\r\n      app: fibonacci-backend-cache\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: fibonacci-backend-cache\r\n    spec:\r\n      terminationGracePeriodSeconds: 60\r\n      volumes:\r\n        - name: cache-volume\r\n          emptyDir:\r\n            medium: Memory\r\n        - name: log-volume\r\n          persistentVolumeClaim:\r\n            claimName: cache-log-storage-claim\r\n      containers:\r\n        - name: fibonacci\r\n          image: chagchagchag/fibonacci-backend-cache:0.0.1\r\n          imagePullPolicy: Always\r\n          volumeMounts:\r\n            - mountPath: /fibonacci/logs\r\n              name: log-volume\r\n            - mountPath: /fibonacci/cache\r\n              name: cache-volume\r\n          env:\r\n            - name: APP_LANGUAGE\r\n              valueFrom:\r\n                configMapKeyRef:\r\n                  name: fibonacci-cache-config\r\n                  key: language\r\n            - name: API_KEY\r\n              valueFrom:\r\n                secretKeyRef:\r\n                  name: fibonacci-cache-secret\r\n                  key: api-key\r\n          lifecycle:\r\n            preStop:\r\n              exec:\r\n                command: [\"/bin/sh\",\"-c\",\"sleep 10\"]\r\n          readinessProbe:\r\n            httpGet:\r\n              path: /probe/healthcheck\r\n              port: 8080\r\n            initialDelaySeconds: 15\r\n            periodSeconds: 1\r\n            successThreshold: 2\r\n            failureThreshold: 3\r\n          livenessProbe:\r\n            httpGet:\r\n              path: /probe/healthcheck\r\n              port: 8080\r\n            initialDelaySeconds: 15\r\n            periodSeconds: 1\r\n            failureThreshold: 7\r\n---\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: fibonacci-backend-cache-service\r\n  namespace: fibonacci\r\nspec:\r\n  selector:\r\n    app: fibonacci-backend-cache\r\n  ports:\r\n    - protocol: TCP\r\n      port: 8080\r\n---\r\napiVersion: networking.k8s.io/v1\r\nkind: Ingress\r\nmetadata:\r\n  name: fibonacci-ingress\r\n  namespace: fibonacci\r\nspec:\r\n  rules:\r\n    - http:\r\n        paths:\r\n          - pathType: Prefix\r\n            path: /fibonacci\r\n            backend:\r\n              service:\r\n                name: fibonacci-backend-cache-service\r\n                port:\r\n                  number: 8080\n이제 kubectl 을 통해 적용해봅니다.\n$ kubectl apply -f fibonacci-cache.yml\r\nnamespace/fibonacci created\r\npersistentvolumeclaim/cache-log-storage-claim created\r\ndeployment.apps/fibonacci-backend-cache-deploy created\r\nservice/fibonacci-backend-cache-service created\r\ningress.networking.k8s.io/fibonacci-ingress created\n생성된 리소스들을 확인해봅니다.\n$ kubectl -n fibonacci get all\r\nNAME                                                  READY   STATUS                       RESTARTS   AGE\r\npod/fibonacci-backend-cache-deploy-57558f6b5d-65kbl   0/1     CreateContainerConfigError   0          32s\r\npod/fibonacci-backend-cache-deploy-57558f6b5d-8274h   0/1     CreateContainerConfigError   0          32s\r\n\r\nNAME                                      TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE\r\nservice/fibonacci-backend-cache-service   ClusterIP   10.96.29.34   <none>        8080/TCP   32s\r\n\r\nNAME                                             READY   UP-TO-DATE   AVAILABLE   AGE\r\ndeployment.apps/fibonacci-backend-cache-deploy   0/2     2            0           32s\r\n\r\nNAME                                                        DESIRED   CURRENT   READY   AGE\r\nreplicaset.apps/fibonacci-backend-cache-deploy-57558f6b5d   2         2         0       32s\n생성한 리소스들을 삭제하려면 아래와 같이 kubectl delete -f  를 해줍니다.\n$ kubectl delete -f fibonacci-cache.yml\r\nnamespace \"fibonacci\" deleted\r\npersistentvolumeclaim \"cache-log-storage-claim\" deleted\r\ndeployment.apps \"fibonacci-backend-cache-deploy\" deleted\r\nservice \"fibonacci-backend-cache-service\" deleted\r\ningress.networking.k8s.io \"fibonacci-ingress\" deleted"}},"/fibonacci-backend-cache/introduce-fibonacci-backend-cache":{"title":"Introduce Fibonacci Backend Cache","data":{"소개#소개":"fibonacci-backend-cache 백엔드 애플리케이션의 개발 과정을 정리하는 카테고리입니다."}},"/fibonacci-backend-cache/backend-cache-perf-test":{"title":"Backend Cache Perf Test","data":{"부하테스트#부하테스트":"Unit 테스트는 아니고, 성능테스트 결과를 문서로 작성합니다.예를 들어, Shell Script 를 통해 1000개 이상의 요청에 대해 load가 어떻게 분산되는지 등 이런 내용들에 대한 스크린 샷 등으로 문서화 합니다."}},"/fibonacci-backend-web/backend-web-graceful-shutdown":{"title":"Backend Web Graceful Shutdown","data":{"graceful-shutdown-처리#Graceful Shutdown 처리":""}},"/fibonacci-backend-cache/backend-cache-kustomize":{"title":"Backend Cache Kustomize","data":{"kustomize-overlay-작업#Kustomize Overlay 작업":"코드작업은 정말 빠르게 하고 쉬웠는데, 문서작업은 왜 이렇게 힘든걸까요? ㅠㅠ","깃헙-리포지터리#깃헙 리포지터리":"여기서 정리하는 내용들은 github.com/chagchagchag/fibonacci-backend 내의 fibonacci-backend/k8s/kustomize 디렉터리에 있는 내용들입니다.","secret#Secret":"fibonacci-backend/k8s/kustomize 디렉터리 내의 create-secret.sh 파일에 대한 내용입니다.kustomize 를 적용하기에 앞서서 아래의 내용을 적용해줍니다.\nkubectl -n fibonacci create secret generic fibonacci-cache-secret --from-literal=api-key=abcd-efgh-ijkl-1111","redis#Redis":"fibonacci-backend/k8s/kustomize 디렉터리 내의 create-redis.sh 파일에 대한 내용입니다.\r\nredis 가 설치되어 있지 않다면 아래의 내용을 적용해줍니다.\nkubectl apply -f redis-service.yml\r\nkubectl apply -f redis-pod.yml","fibonacci-backend-cache-의-kustomize-작업#fibonacci-backend-cache 의 kustomize 작업":"대략적인 구조는 이렇습니다.\r\nbase 디렉터리\n기본적인 리소스 들의 정의 파일들을 둡니다.\n이렇게 정의된 파일들을 base 디렉터리 내에서 kustomize 하는 역할을 하는 파일은 kustomization.yml 입니다.\noverlay 디렉터리\noverlay/develop 디렉터리\nbase/kustomization.yml 파일에서 조합한 리소스파일 들에서 tag, namespace를 develop 에 맞게끔 덮어쓰는 작업을 합니다.\noverlay/develop/kustomization.yml 파일에서 따로 develop 버전에 맞도록 재정의한 namespace, tag 등이 적용되어 재정의됩니다.\noverlay/production 디렉터리\nbase/kustomization.yml 파일에서 조합한 리소스파일 들에서 tag, namespace를 production 에 맞게끔 덮어쓰는 작업을 합니다.\noverlay/production/kustomization.yml 파일에서 따로 production 버전에 맞도록 재정의한 namespace, tag 등이 적용되어 재정의됩니다.\n이렇게 작성한 파일들은 아래와 같이 적용합니다.\r\ndevelop Phase 에 배포할 경우\n$ cd k8s/kustomize/fibonacci-backend-cache\r\n$ cd overlay/develop\r\n$ kubectl kustomize ./ | kubectl apply -f -\nproduction Phase 에 배포할 경우\n$ cd k8s/kustomize/fibonacci-backend-cache\r\n$ cd overlay/develop\r\n$ kubectl kustomize ./ | kubectl apply -f -\nkubectl kustomize ./ 명령은 현재 디렉터리 내의 kustomization.yml 파일에 정의한 리소스들을 기준으로 각각의 리소스를 조합해서 리소스 정의서를 만들어냅니다. 이렇게 해서 명령창에서 kubectl kustomize ./ 을 실행하면 리소스 정의 yml 이 생성되니다.kubectl apply -f 앞에 붙은 - 의 의미는 표준 입력을 의미합니다. kubectl kustomize ./ 을 통해서 조합한 리소스 정의에 대한 yaml 문자열이 입출력 파이프라인 명령어인 | 을 통해 유입되고 이 파이프라인을 통해 kubectl kustomize ./ 명령어에 인자값으로 전달됩니다.","리소스-파일들#리소스 파일들":"","base#base":"","basekustomizationyml#base/kustomization.yml":"apiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nresources:\r\n- fibonacci-cache-namespace.yml\r\n- fibonacci-cache-ingress.yml\r\n- fibonacci-cache-config.yml\r\n- fibonacci-cache-local-storage.yml\r\n- fibonacci-cache-log-pvc.yml\r\n- fibonacci-cache-deploy.yml\r\n- fibonacci-cache-service.yml","basefibonacci-cache-configyml#base/fibonacci-cache-config.yml":"apiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: fibonacci-cache-config\r\n  namespace: fibonacci\r\ndata:\r\n  language: \"ko\"","basefibonacci-cache-ingressyml#base/fibonacci-cache-ingress.yml":"apiVersion: networking.k8s.io/v1\r\nkind: Ingress\r\nmetadata:\r\n  name: fibonacci-ingress\r\n  namespace: fibonacci\r\nspec:\r\n  rules:\r\n    - http:\r\n        paths:\r\n          - pathType: Prefix\r\n            path: /fibonacci\r\n            backend:\r\n              service:\r\n                name: fibonacci-backend-cache-service\r\n                port:\r\n                  number: 8080","basefibonacci-cache-serviceyml#base/fibonacci-cache-service.yml":"apiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: fibonacci-backend-cache-service\r\n  namespace: fibonacci\r\nspec:\r\n  selector:\r\n    app: fibonacci-backend-cache\r\n  ports:\r\n    - protocol: TCP\r\n      port: 8080","basefibonacci-cache-deployyml#base/fibonacci-cache-deploy.yml":"apiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: fibonacci-backend-cache-deploy\r\n  namespace: fibonacci\r\nspec:\r\n  replicas: 2\r\n  strategy:\r\n    type: RollingUpdate\r\n    rollingUpdate:\r\n      maxSurge: 1\r\n      maxUnavailable: 0\r\n  selector:\r\n    matchLabels:\r\n      app: fibonacci-backend-cache\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: fibonacci-backend-cache\r\n    spec:\r\n      terminationGracePeriodSeconds: 60\r\n      volumes:\r\n        - name: cache-volume\r\n          emptyDir:\r\n            medium: Memory\r\n        - name: log-volume\r\n          persistentVolumeClaim:\r\n            claimName: cache-log-storage-claim\r\n      containers:\r\n        - name: fibonacci\r\n          image: chagchagchag/fibonacci-backend-cache:0.0.1\r\n          imagePullPolicy: Always\r\n          volumeMounts:\r\n            - mountPath: /fibonacci/logs\r\n              name: log-volume\r\n            - mountPath: /fibonacci/cache\r\n              name: cache-volume\r\n          env:\r\n            - name: APP_LANGUAGE\r\n              valueFrom:\r\n                configMapKeyRef:\r\n                  name: fibonacci-cache-config\r\n                  key: language\r\n            - name: API_KEY\r\n              valueFrom:\r\n                secretKeyRef:\r\n                  name: fibonacci-cache-secret\r\n                  key: api-key\r\n          lifecycle:\r\n            preStop:\r\n              exec:\r\n                command: [\"/bin/sh\",\"-c\",\"sleep 10\"]\r\n          readinessProbe:\r\n            httpGet:\r\n              path: /probe/healthcheck\r\n              port: 8080\r\n            initialDelaySeconds: 15\r\n            periodSeconds: 1\r\n            successThreshold: 2\r\n            failureThreshold: 3\r\n          livenessProbe:\r\n            httpGet:\r\n              path: /probe/healthcheck\r\n              port: 8080\r\n            initialDelaySeconds: 15\r\n            periodSeconds: 1\r\n            failureThreshold: 7","basefibonacci-cache-namespaceyml#base/fibonacci-cache-namespace.yml":"apiVersion: v1\r\nkind: Namespace\r\nmetadata:\r\n  name: fibonacci-cache","basefibonacci-cache-log-pvcyml#base/fibonacci-cache-log-pvc.yml":"apiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: cache-log-storage-claim\r\n  namespace: fibonacci\r\nspec:\r\n  # kubectl get sc 명령을 통해 나타나는 StorageClass 중 하나를 선택했다.\r\n  # 로컬에서는 kind 클러스터 버전에 따라 standard 가 나올수도 있고 local-storage 가 나올수도 있다.\r\n  storageClassName: standard\r\n  accessModes:\r\n    - ReadWriteOnce\r\n  resources:\r\n    requests:\r\n      storage: 100Mi","overlay#overlay":"overlay 에 정의하는 kustomization.yml 에는 주로 배포 Phase 에 맞는 환경, 이미지 명, 태그, 볼륨 명, 각종 환경 변수 등을 덮어쓰는 내용 들을 정의합니다.","overlaydevelopkustomizationyml#overlay/develop/kustomization.yml":"apiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nresources:\r\n  - ../../base\r\nimages:\r\n  - name: chagchagchag/fibonacci-backend-cache\r\n  - newName: chagchagchag/fibonacci-backend-cache\r\n  - newTag: 0.0.1-fibonacci-backend-cache.01\r\nnamespace: fibonacci-cache-develop","overlayproductionkustomizationyml#overlay/production/kustomization.yml":"apiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nresources:\r\n  - ../../base\r\nimages:\r\n  - name: chagchagchag/fibonacci-backend-cache\r\n  - newName: chagchagchag/fibonacci-backend-cache\r\n  - newTag: 0.0.1-fibonacci-backend-cache.01\r\nnamespace: fibonacci-cache-production"}},"/fibonacci-backend-web/backend-web-hpa-testing":{"title":"Backend Web Hpa Testing","data":{"hpa-정의-및-테스트#HPA 정의 및 테스트":"kustomize 를 적용해서 hpa 적용버전, 일반 deployment 를 적용할수도 있겠지만, 이렇게 하면 kustomize 까지 설명해야 해서 이번 문서인 HPA테스트 문서는 일반적인 yaml 파일로 HPA를 정의하고 테스트했습니다.잠시 수정 중입니다. 내용을 깔끔하게 가다듬는 데에 3시간 정도 소요될것 같습니다. 내용이 계속해서 바뀌더라도 양해부탁드립니다.","예제-적용범위-예제-스크립트-경로#예제 적용범위, 예제 스크립트 경로":"예제 적용범위이번 예제는 fibonacci-backend-web 에 범위를 한정했습니다.fibonacci-backend-cache 까지 적용해서 테스트하기에는 문서의 범위가 너무 길어져서 fibonacci-backend-web 까지로 범위를 한정했습니다.예제 스크립트이번 예제에서 사용한 성능 테스트를 위한 리소스 정의 파일 및 쉘 스크립트들은 아래 링크에서 모두 찾아볼 수 있습니다.\ngithub.com/fibonacci-backend/k8s/plain-deploy/fibonacci-backend-web/test","테스트-영상#테스트 영상":"테스트해본 영상은 길이가 다소 길고 Github 에 업로드할 수 없어서 Youtube에 업로드해두었고 아래의 링크를 통해서만 시청이 가능합니다.\nHPA 를 적용하지 않은 단순 디플로이먼트의 동작\nHPA 가 적용된 디플로이먼트\n이번 문서에서는 위 영상들의 시연 영상을 캡처한 이미지들을 기반으로 HPA 적용시에 어떻게 달라지고, HPA 적용전에는 어떤지를 설명합니다.","테스트-시나리오#테스트 시나리오":"HPA 가 잘 적용되는지 테스트를 해보는 테스트 시나리오는 아래와 같습니다. 꼭 HPA 의 동작을 확인한 후에는 다시 정상으로 돌려두어야 합니다.로드 유발 쉘스크립트 작성\n테스트를 위해 generate-load-request.sh 에서는 0.5초마다 200개의 request 를 보내는데 http://localhost/fibonacci?number=11 을 단순 GET 요청으로 보냅니다.\nSpring Boot 애플리케이션에 지연코드 추가\n테스트를 위해 지연이 발생하는 코드를 쿠버네티스 애플리케이션에 추가합니다.\nfibonacci-backend-web 내에서 fibonacci 계산을 수행하는 함수에 500ms 의 delay 를 부여합니다.\n애플리케이션에서는 API 에 대해 수행하는 연산이 500ms 의 시간이 걸린다는 것을 가정합니다. 500ms 는 애플리케이션 입장에서는 꽤 큰 지연시간입니다.\nspring boot 애플리케이션의 톰캣 스레드 풀 사이즈 수정\n스레드 풀을 10개만 있는 상황을 가정합니다.\n이렇게 하면 하나의 파드는 1초에 2번의 요청을 처리할 수 있는 API를 10개의 스레드에서 운영하게 되므로 1초에 10개의 스레드를 이용해 20번의 요청을 처리할 수 있습니다. 이런 상황을 가정해서 테스트를 수행하겠습니다.\nHPA 적용 전 일반 디플로이먼트 테스트\n일반 디플로이먼트를 테스트하기 위한 리소스 정의 파일을 작성합니다.\n테스트를 위한 파드의 성능 축소, Probe 재시동 판단 조건 완화\nCPU, Memory 를 조금은 작은 용량으로 준비해둡니다. 스케일링이 잘 되는지 확인해보기 위해서입니다.\n오토스케일링 테스트를 위해 readinessProbe, livenessProbe 를 통해 파드의 재시동 주기를 조절합니다. 가급적 재시동이 자주 걸리지 않도록 변경해줍니다.\nreadiness 프로브에서는 successThreshold를 작게주어서 재기동 중에 다시 재기동이 이뤄지지 않도록 하고, failureThreshold 는 더 크게 주어서 관대하게 설정을 합니다.\nliveness 프로브에는 failureThreshold 를 조정해서 live 판정이 조금 더 자주 일어나지 않도록 조정합니다.\nHPA 적용 및 테스트\n재기동이 민감하지 않은 것을 확인했으면 이제는 HPA 설정을 시작합니다. minReplicas=2, maxReplicas=5 로 주어서 2~5 사이에서 스케일링이 이뤄지게끔 합니다. averageUtilization 은 50 으로 주어서 1000m * 50% 인 500m 일때 스케일링이 일어나도록 설정해줍니다. 그리고 stabilizationWindowSeconds 를 지정해서 스케일링 주기를 결정합니다.\n스케일링이 이뤄지는지 generate-load-request.sh 를 통해 확인해봅니다.","사전-준비#사전 준비":"","클러스터-생성#클러스터 생성":"kind 클러스터가 로컬 개발환경에 설치되어 있지 않은 상태인 경우 아래와 같이 kind 클러스터를 설치해줍니다.\ncd cluster\r\nsource create-cluster.sh","redis-설치#redis 설치":"cd k8s/plain-deploy/fibonacci-common\r\nkubectl apply -f redis.yml\nredis.yml 의 yaml 파일 내용은 아래와 같고 github.com/chaghchagchag/fibonacci-bacend/redis.yml 에서 확인 가능합니다.\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: redis-service\r\nspec:\r\n  selector:\r\n    app: redis\r\n  ports:\r\n    - protocol: TCP\r\n      port: 6379\r\n      targetPort: 6379\r\n---\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: redis\r\n  labels:\r\n    app: redis\r\nspec:\r\n  containers:\r\n    - name: redis\r\n      image: redis\r\n      ports:\r\n        - containerPort: 6379","로드-유발-쉘스크립트-작성#로드 유발 쉘스크립트 작성":"","metrics-server-설치#metrics-server 설치":"metrics-server 는 k8s 플랫폼마다 기본으로 설치된 곳도 있고 아닌 플랫폼도 있습니다. 로컬 클러스터인 kind 클러스터에서도 설치가 되어있는지 확인이 필요합니다.클러스터 내에 metrics-server가 설치되었는지 확인을 합니다.\n$ kubectl top nodes\r\nerror: Metrics API not available\nmetrics-server 가 설치되어 있지 않습니다. metrics-server 에 방문해서 metrics-server를 설치합니다.metrics-server#installation에서는 metrics-server 를 설치하는 명령어를 알려주고 있습니다. 이 명령어를 복사해서 쿠버네티스 클러스터에 적용해줍니다.\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n이렇게 설치를 해도 아래와 같이 kubectl top nodes 로 현황을 확인해도 에러 메시지가 나타나는 경우가 있습니다.\n$ kubectl top nodes\r\nerror: Metrics API not available\n이 경우 아래와 같이 metrics-server 라는 이름의 deployment 를 수정하는 명령어를 실행해줍니다.\nkubectl edit deployments -n kube-system metrics-server\n그리고 나타나는 시스템 에디터에서는 --kubelet-insecure-tls 옵션을 추가해준 후 저장합니다.\r\n설치가 잘 되었는지 확인해봅니다. 결과를 보니 잘 설치되었습니다. (제일 아래줄에 metrics-server-796fbd6c9d-29hdz 을 확인 가능)\n$ kubectl -n kube-system get pods\r\nNAME                                                      READY   STATUS    RESTARTS   AGE\r\ncoredns-5d78c9869d-hx9td                                  1/1     Running   0          48m\r\ncoredns-5d78c9869d-stjkw                                  1/1     Running   0          48m\r\netcd-fibonacci-cluster-control-plane                      1/1     Running   0          48m\r\nkindnet-rzptz                                             1/1     Running   0          48m\r\nkindnet-vhxbz                                             1/1     Running   0          48m\r\nkube-apiserver-fibonacci-cluster-control-plane            1/1     Running   0          48m\r\nkube-controller-manager-fibonacci-cluster-control-plane   1/1     Running   0          48m\r\nkube-proxy-m46jh                                          1/1     Running   0          48m\r\nkube-proxy-mtrtz                                          1/1     Running   0          48m\r\nkube-scheduler-fibonacci-cluster-control-plane            1/1     Running   0          48m\r\nmetrics-server-796fbd6c9d-29hdz                           1/1     Running   0          23s\nkubectl top nodes 명령을 통해 지표들이 수집되는지 확인해봅니다.\n$ kubectl top nodes\r\nNAME                              CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\r\nfibonacci-cluster-control-plane   144m         0%     1213Mi          6%\r\nfibonacci-cluster-worker          29m          0%     283Mi           1%","curl-요청-쉘스크립트-추가#curl 요청 쉘스크립트 추가":"단순한 코드입니다. 1초에 한번씩 50개의 curl 요청을 GET http://localhost:8080/number=1 으로 보내는 쉘스크립트입니다.\n#!/bin/bash\r\n\r\nwhile true; do    \r\n  count=50\r\n\r\n  for ((i = 0; i < count; i++)); do\r\n    curl \"http://localhost:8080/fibonacci?number=1\" &\r\n  done\r\n \r\n  sleep 1  \r\ndone","spring-boot--애플리케이션에-지연코드-추가#spring boot  애플리케이션에 지연코드 추가":"이번 테스트 실습이 끝난 후에는 반드시 지연코드를 지우고 원래의 코드로 되돌려놓아야 합니다.\nlocalhost:8080/fibonacci API 수행시 500ms 만큼 지연이 계속해서 발생하는 상황을 가정한 delay 코드를 추가합니다.추가한 코드 옆 또는 코드 위에 주석을 추가해두었습니다.\n@Service\r\npublic class FibonacciService {\r\n    \r\n    // ... \r\n    \r\n    public BigDecimal getFibonacci(int number) {\r\n        delayMs(500); // 여기에 추가\r\n        if(number == 0) return BigDecimal.ZERO;\r\n        else if(number == 1) return BigDecimal.ONE;\r\n        else if(number == 2) return BigDecimal.ONE;\r\n        else{\r\n            if(dp.containsKey(number)) return dp.get(number);\r\n\r\n            dp.put(number, getFibonacci(number-2).add(getFibonacci(number-1)));\r\n\r\n            return dp.get(number);\r\n        }\r\n    }\r\n\r\n    // 여기이 코드를 보시면 됩니다. \r\n    public void delayMs(long ms){\r\n        long spentTime = 0L;\r\n        long started = System.currentTimeMillis();\r\n        do {\r\n            spentTime = System.currentTimeMillis() - started;\r\n        } while(spentTime < ms);\r\n    }\r\n    \r\n    // ...\r\n    \r\n}","spring-boot-애플리케이션의-톰캣-스레드-풀-사이즈-수정#spring boot 애플리케이션의 톰캣 스레드 풀 사이즈 수정":"이번 테스트 실습이 끝난 후에는 반드시 원래의 톰캣 스레드 풀 설정으로 되돌려줘야 합니다.\n스프링 컨테이너의 커넥션을 처리하는 스레드가 부족해지는 상황을 가정하기 위해 스프링 부트 톰캣 스레드 풀의 사이즈를 10으로 수정합니다. 스프링 부트 톰캣 스레드 풀의 기본사이즈는 200 입니다.fibonacci-backend-web/src/resources/application.yml\nserver.tomcat.threads.max 를 10 으로 설정해주었습니다.\nserver:\r\n  shutdown: graceful\r\n  tomcat:\t\t\t## 여기에 추가해주었습니다.\r\n    threads:\t\t## 여기에 추가해주었습니다.\r\n      max: 10\t\t## 여기에 추가해주었습니다.\r\n\r\nspring:\r\n  lifecycle:\r\n    timeout-per-shutdown-phase: 50s\r\n  data:\r\n    redis:\r\n      host: redis-server.default.svc.cluster.local\r\n      port: 6379\r\n\r\n#logging:\r\n#  file:\r\n#    name: /var/log/app/fibonacci.log","테스트를-위한-파드의-성능-축소-probe-재시동-판단-조건-완화#테스트를 위한 파드의 성능 축소, Probe 재시동 판단 조건 완화":"","cpu-memory-의-requests-limits-수정#CPU, Memory 의 requests, limits 수정":"이번 테스트 실습이 끝난 후에는 반드시 원래의 CPU, Memory 설정으로 되돌려줘야 합니다.\n테스트를 위해 파드의 스펙을 축소해서 아래와 같이 수정해줍니다.\nmemory\nrequests : \"512Mi\"\nlimits : \"1Gi\"\ncpu\nrequests: \"1000m\"\nlimits: \"1500m\"\n수정해준 부분은 아래에 ## *** 여기를 수정함 이라는 주석으로 명시해두었습니다.hpa 코드 (k8s/plain-deploy/fibonacci-backend-web-deploy.yml)\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: fibonacci-backend-web-deploy\r\n  namespace: fibonacci\r\nspec:\r\n  replicas: 2\r\n  strategy:\r\n    type: RollingUpdate\r\n    rollingUpdate:\r\n      maxSurge: 1\r\n      maxUnavailable: 0\r\n  selector:\r\n    matchLabels:\r\n      app: fibonacci-backend-web\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: fibonacci-backend-web\r\n    spec:\r\n      terminationGracePeriodSeconds: 60\r\n      containers:\r\n        - name: fibonacci\r\n          image: chagchagchag/fibonacci-backend-web:0.0.1\r\n          imagePullPolicy: Always\r\n          ## 여기서부터 시작\r\n          resources:\r\n            requests:\r\n              memory: \"512Mi\" ## *** 여기를 수정함\r\n              cpu: \"1000m\"    ## *** 여기를 수정함\r\n            limits:\r\n              memory: \"1Gi\"   ## *** 여기를 수정함\r\n              cpu: \"1500m\"    ## *** 여기를 수정함\r\n\r\n## ...","probe-재시동-판단-조건-완화#Probe 재시동 판단 조건 완화":"테스트가 끝난 후에는 반드시 원래의 Probe 설정으로 되돌려줘야 합니다.\n성능을 축소했기에 파드가 재시동될 가능성이 높습니다. 재시동으로 인한 CPU 점유율이 올라갈 때 축소한 성능을 넘어서는 경우가 있어서 재기동이 계속해서 생길 수 있는데 이 부분에 대해서 Probe 로 재시동 요건을 완화해줍니다. 이렇게 성능을 축소한 후에 재기동 횟수를 최소화 하는 이유는 HPA 가 정상적으로 수행되는지를 확인하기 위해서입니다.","readiness-probe-설정-완화#Readiness Probe 설정 완화":"테스트가 끝난 후에는 반드시 원래의 Probe 설정으로 되돌려줘야 합니다.\n전체 코드는 github.com/chagchagchag/fibonacci-backend/k8s/plain-deploy/fibonacci-backend-web/fibonacci-web-deploy.yml 을 참고하시면 됩니다.\nsuccessThreshold = 2, failureThreshold = 3 으로 되어 있던 것을 successThreshold = 1, failureThreshold = 2 로 변경해줬습니다.\n원래 설정은 두번 성공해야 네트워크를 허용하고, 세번 실패해야 네트워크를 끊는 설정이었는데 이것을 변경해서 한번만 성공해도 네트워크에 접속이 가능하게 하고 두번 실패했을 때에 네트워크를 끊도록 하는 설정으로 변경해주었습니다.\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: fibonacci-backend-web-deploy\r\n  namespace: fibonacci\r\nspec:\r\n  replicas: 2\r\n  strategy:\r\n    type: RollingUpdate\r\n    rollingUpdate:\r\n      maxSurge: 1\r\n      maxUnavailable: 0\r\n  selector:\r\n    matchLabels:\r\n      app: fibonacci-backend-web\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: fibonacci-backend-web\r\n    spec:\r\n      terminationGracePeriodSeconds: 60\r\n      containers:\r\n        - name: fibonacci\r\n          image: chagchagchag/fibonacci-backend-web:0.0.1\r\n          \r\n          ## ...\r\n          \r\n          readinessProbe:\r\n            httpGet:\r\n              path: /probe/ready\r\n              port: 8080\r\n            initialDelaySeconds: 45\r\n            periodSeconds: 1\r\n            successThreshold: 1\r\n            failureThreshold: 2\r\n          \r\n          ## ...","liveness-probe-설정-완화#Liveness Probe 설정 완화":"테스트가 끝난 후에는 반드시 원래의 Probe 설정으로 되돌려줘야 합니다.\n전체 코드는 github.com/chagchagchag/fibonacci-backend/k8s/plain-deploy/fibonacci-backend-web/fibonacci-web-deploy.yml 을 참고하시면 됩니다.\nfailureThreshold 가 3이었던 것을 10으로 변경했습니다.\nlivenessProbe의 경우 컨테이너 자체를 restart 하는 것으로 인해 장기간 파드가 작업을 처리하지 못하도록 하는 문제가 있기 때문에 원래 3 이었던 failureThreshold 를 10 으로 늘려서 되도록이면 restart 필요 없이 readinessProbe 만으로 트래픽 양을 제어하면서 트래픽을 처리할 수 있도록 변경해줬다.\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: fibonacci-backend-web-deploy\r\n  namespace: fibonacci\r\nspec:\r\n  replicas: 2\r\n  strategy:\r\n    type: RollingUpdate\r\n    rollingUpdate:\r\n      maxSurge: 1\r\n      maxUnavailable: 0\r\n  selector:\r\n    matchLabels:\r\n      app: fibonacci-backend-web\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: fibonacci-backend-web\r\n    spec:\r\n      terminationGracePeriodSeconds: 60\r\n      containers:\r\n        - name: fibonacci\r\n          image: chagchagchag/fibonacci-backend-web:0.0.1\r\n          \r\n          ## ...\r\n          \r\n          livenessProbe:\r\n            httpGet:\r\n              path: /probe/live\r\n              port: 8080\r\n            initialDelaySeconds: 45\r\n            periodSeconds: 1\r\n            failureThreshold: 10 ### 이곳을 수정","낮춘-서버-사양에-맞도록-각-프로브의-initialdelayseconds-수정#낮춘 서버 사양에 맞도록 각 프로브의 initialDelaySeconds 수정":"서버사양을 낮춘 만큼 부팅이 늦어질 가능성이 높기 때문에 Redyness 프로브, Startup 프로브의 initialDelaySeconds 를 아래와 같이 45 로 수정해줍니다.\n## ...\r\n          startupProbe:\r\n            httpGet:\r\n              path: /probe/startup\r\n              port: 8080\r\n            initialDelaySeconds: 45\r\n            periodSeconds: 1\r\n            failureThreshold: 10\r\n          readinessProbe:\r\n            httpGet:\r\n              path: /probe/ready\r\n              port: 8080\r\n            initialDelaySeconds: 45\r\n            periodSeconds: 1\r\n            successThreshold: 1\r\n            failureThreshold: 2\r\n          livenessProbe:\r\n            httpGet:\r\n              path: /probe/live\r\n              port: 8080\r\n            initialDelaySeconds: 45\r\n            periodSeconds: 1\r\n            failureThreshold: 10","hpa-적용-및-테스트#HPA 적용 및 테스트":"","kubectl--n-fibonacci-get-pods-를-1초-단위로-모니터링#kubectl -n fibonacci get pods 를 1초 단위로 모니터링":"터미널에 아래의 명령어를 수행해서 1초에 한번씩 pod 의 상태를 모니터링합니다.\nwatch -n 1 kubectl -n fibonacci get pods\n이렇게 하면 watch -n 1 명령은 1초에 한번씩(-n 1) watch 를 하겠다는 의미이고, kubectl get pod 를 1초에 한번씩 수행해서 터미널에 출력하게 됩니다.","kubectl--n-fibonacci-get-top-pods-를-1초-단위로-모니터링#kubectl -n fibonacci get top pods 를 1초 단위로 모니터링":"터미널에 아래의 명령어를 수행해서 1초에 한번씩 pod의 CPU, MEMORY 현황을 모니터링합니다.\nwatch -n 1 kubectl -n fibonacci top pods\n이렇게 하면 watch -n 1 명령은 1초에 한번씩(-n 1) watch 를 하겠다는 의미이고, kubectl top pods 를 1초에 한번씩 수행해서 터미널에 출력하게 됩니다.","generate-load-requestsh-실행#generate-load-request.sh 실행":"이 문서의 초입에서 작성했던 generate-load-request.sh을 실행합니다.\nsource generate-load-request.sh","hpa-적용-전후-테스트#HPA 적용 전후 테스트":"","hpa-적용-전#HPA 적용 전":"스크린샷 추가 예정 (순간포착이 쉽지 않아서 따로 캡처작업을 수행 예정)","hpa-적용#HPA 적용":"pod 를 2 ~ 5 개로 성능 지표에 따라서 스케일업이 발생하도록 리소스를 정의한 코드는 아래와 같습니다.\napiVersion: autoscaling/v2\r\nkind: HorizontalPodAutoscaler\r\nmetadata:\r\n  name: fibonacci-backend-hpa\r\n  namespace: fibonacci\r\nspec:\r\n  minReplicas: 2\r\n  maxReplicas: 5\r\n  scaleTargetRef:\r\n    apiVersion: apps/v1\r\n    kind: Deployment\r\n    name: fibonacci-backend-web\r\n  metrics:\r\n    - type: Resource\r\n      resource:\r\n        name: cpu\r\n        target:\r\n          type: Utilization\r\n          averageUtilization: 50\r\n  behavior:\r\n    scaleUp:\r\n      stabilizationWindowSeconds: 30\r\n    scaleDown:\r\n      stabilizationWindowSeconds: 30\nspec.minReplicas, spec.maxReplicas\nmin : 2, max: 5 로 해서 최소 2기의 파드로 운영되고 부하가 심할 때는 최대 5기의 파드로 운영되게끔 지정해줬습니다.\nspec:\r\n  minReplicas: 2\r\n  maxReplicas: 5\r\n  # ...\nspec.metrics ...\nResource 타입의 메트릭을 지정해줬고, cpu 자원에 대해 Utilization 을 적용해주었고 averageUtilization 을 50 으로 지정해줬습니다.\naverageUtilization 에 대해서는 쿠버네티스의 스케일링 문서에 설명을 정리해두었습니다.\nspec:\r\n  # ... \r\n  metrics:\r\n    - type: Resource\r\n      resource:\r\n        name: cpu\r\n        target:\r\n          type: Utilization\r\n          averageUtilization: 50","hpa-적용-후#HPA 적용 후":"스크린샷 추가 예정 (순간포착이 쉽지 않아서 따로 캡처작업을 수행 예정)"}},"/fibonacci-backend-web/backend-web-coding":{"title":"Backend Web Coding","data":{"backend-코드-작업#Backend 코드 작업":"","fibonaccicontroller#FibonacciController":"@RestController\r\npublic class FibonacciController {\r\n\r\n    private final FibonacciService fibonacciService;\r\n\r\n    public FibonacciController(FibonacciService fibonacciService){\r\n        this.fibonacciService = fibonacciService;\r\n    }\r\n\r\n    @GetMapping(\"/fibonacci\")\r\n    public ResponseEntity<BigDecimal> getFibonacci(\r\n            @RequestParam(\"number\") int number\r\n    ){\r\n        return ResponseEntity\r\n                .status(HttpStatus.OK)\r\n                .body(fibonacciService.getFibonacci(number));\r\n    }\r\n\r\n}","fibonacciservice#FibonacciService":"@Service\r\npublic class FibonacciService {\r\n\r\n    private final StringRedisTemplate stringRedisTemplate;\r\n\r\n    public FibonacciService(StringRedisTemplate stringRedisTemplate){\r\n        this.stringRedisTemplate = stringRedisTemplate;\r\n    }\r\n\r\n    private final Map<Integer, BigDecimal> dp = new HashMap<>();\r\n    private static final String QUEUE_HASH_KEY = \"fibonacci:task-queue\";\r\n    private static final String SET_HASH_KEY = \"fibonacci:result-set\";\r\n\r\n    public BigDecimal getFibonacci(int number) {\r\n        if(number == 0) return BigDecimal.ZERO;\r\n        else if(number == 1) return BigDecimal.ONE;\r\n        else if(number == 2) return BigDecimal.ONE;\r\n        else{\r\n            if(dp.containsKey(number)) return dp.get(number);\r\n\r\n            dp.put(number, getFibonacci(number-2).add(getFibonacci(number-1)));\r\n\r\n            return dp.get(number);\r\n        }\r\n    }\r\n\r\n    // **TODO** : 'fibonacci-backend-batch' 로 아래 기능을 이관 예정.\r\n    @Scheduled(fixedDelay = 1000L)\r\n    public void scheduledCalculateFibonacci(){\r\n        if(Boolean.TRUE.equals(stringRedisTemplate.hasKey(QUEUE_HASH_KEY))){\r\n            Optional\r\n                .ofNullable(stringRedisTemplate.opsForSet().pop(QUEUE_HASH_KEY))\r\n                .ifPresent(cachedRequest -> {\r\n                    BigDecimal result = getFibonacci(Integer.parseInt(cachedRequest));\r\n                    stringRedisTemplate.opsForHash()\r\n                            .put(\r\n                                SET_HASH_KEY,\r\n                                cachedRequest,\r\n                                result.toPlainString()\r\n                            );\r\n                });\r\n        }\r\n    }\r\n\r\n}","probe-헬스체크---healthcheckcontroller#Probe (헬스체크) - HealthCheckController":"probe 용도의 Controller 입니다. Probe 관련된 개념들은 이 깃헙 문서 내의 별도의 문서에 따로 개념정리를 해둘 예정입니다.\n@RestController\r\n@RequestMapping(\"/probe\")\r\npublic class HealthCheckController {\r\n\r\n    private final Logger logger = LoggerFactory.getLogger(HealthCheckController.class);\r\n\r\n    @GetMapping(\"/startup\")\r\n    public String startupCheck(){\r\n        logger.info(\"[startup probe] >>> OK\");\r\n        return \"START UP OK\";\r\n    }\r\n\r\n    @GetMapping(\"/ready\")\r\n    public String readinessCheck(){\r\n        logger.info(\"[readiness probe] >>> OK\");\r\n        return \"READY OK\";\r\n    }\r\n\r\n    @GetMapping(\"/live\")\r\n    public String livenessCheck(){\r\n        logger.info(\"[liveness probe] >>> OK\");\r\n        return \"OK\";\r\n    }\r\n\r\n}"}},"/fibonacci-backend-web/backend-web-kustomize":{"title":"Backend Web Kustomize","data":{"kustomize-overlay-작업#Kustomize Overlay 작업":"프로젝트 Github\nkustomize/fibonacci-backend-web","fibonacci-backend-cache-의-kustomize-작업#fibonacci-backend-cache 의 kustomize 작업":"대략적인 구조는 이렇습니다.\r\nbase 디렉터리\n기본적인 리소스 들의 정의 파일들을 둡니다.\n이렇게 정의된 파일들을 base 디렉터리 내에서 kustomize 하는 역할을 하는 파일은 kustomization.yml 입니다.\noverlay 디렉터리\noverlay/develop 디렉터리\nbase/kustomization.yml 파일에서 조합한 리소스파일 들에서 tag, namespace를 develop 에 맞게끔 덮어쓰는 작업을 합니다.\noverlay/develop/kustomization.yml 파일에서 따로 develop 버전에 맞도록 재정의한 namespace, tag 등이 적용되어 재정의됩니다.\noverlay/production 디렉터리\nbase/kustomization.yml 파일에서 조합한 리소스파일 들에서 tag, namespace를 production 에 맞게끔 덮어쓰는 작업을 합니다.\noverlay/production/kustomization.yml 파일에서 따로 production 버전에 맞도록 재정의한 namespace, tag 등이 적용되어 재정의됩니다.\n이렇게 작성한 파일들은 아래와 같이 적용합니다.\r\ndevelop Phase 에 배포할 경우\n$ cd k8s/kustomize/fibonacci-backend-web\r\n$ cd overlay/develop\r\n$ kubectl kustomize ./ | kubectl apply -f -\n이렇게 작성한 파일들은 아래와 같이 적용합니다.\r\ndevelop Phase 에 배포할 경우\n$ cd k8s/kustomize/fibonacci-backend-web\r\n$ cd overlay/develop\r\n$ kubectl kustomize ./ | kubectl apply -f -\nproduction Phase 에 배포할 경우\n$ cd k8s/kustomize/fibonacci-backend-web\r\n$ cd overlay/develop\r\n$ kubectl kustomize ./ | kubectl apply -f -\nkubectl kustomize ./ 명령은 현재 디렉터리 내의 kustomization.yml 파일에 정의한 리소스들을 기준으로 각각의 리소스를 조합해서 리소스 정의서를 만들어냅니다. 이렇게 해서 명령창에서 kubectl kustomize ./ 을 실행하면 리소스 정의 yml 이 생성되니다.kubectl apply -f 앞에 붙은 - 의 의미는 표준 입력을 의미합니다. kubectl kustomize ./ 을 통해서 조합한 리소스 정의에 대한 yaml 문자열이 입출력 파이프라인 명령어인 | 을 통해 유입되고 이 파이프라인을 통해 kubectl kustomize ./ 명령어에 인자값으로 전달됩니다.","리소스-파일들#리소스 파일들":"","base#base":"","basekustomizationyml#base/kustomization.yml":"apiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nresources:\r\n- fibonacci-web-namespace.yml\r\n- fibonacci-web-hpa.yml\r\n- fibonacci-web-deploy.yml\r\n- fibonacci-web-service.yml","basefibonacci-web-deployyml#base/fibonacci-web-deploy,yml":"apiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: fibonacci-backend-web-deploy\r\n  namespace: fibonacci\r\nspec:\r\n  replicas: 2\r\n  strategy:\r\n    type: RollingUpdate\r\n    rollingUpdate:\r\n      maxSurge: 1\r\n      maxUnavailable: 0\r\n  selector:\r\n    matchLabels:\r\n      app: fibonacci-backend-web\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: fibonacci-backend-web\r\n    spec:\r\n      terminationGracePeriodSeconds: 60\r\n      containers:\r\n        - name: fibonacci\r\n          image: chagchagchag/fibonacci-backend-web:0.0.1\r\n          imagePullPolicy: Always\r\n          resources:\r\n            requests:\r\n              memory: \"512Mi\"\r\n              cpu: \"1000m\"\r\n            limits:\r\n              memory: \"1Gi\"\r\n              cpu: \"1500m\"\r\n          lifecycle:\r\n            preStop:\r\n              exec:\r\n                command: [\"/bin/sh\",\"-c\",\"sleep 10\"]\r\n          startupProbe:\r\n            httpGet:\r\n              path: /probe/startup\r\n              port: 8080\r\n            initialDelaySeconds: 45\r\n            periodSeconds: 1\r\n            failureThreshold: 10\r\n          readinessProbe:\r\n            httpGet:\r\n              path: /probe/ready\r\n              port: 8080\r\n            initialDelaySeconds: 45\r\n            periodSeconds: 1\r\n            successThreshold: 1\r\n            failureThreshold: 2\r\n          livenessProbe:\r\n            httpGet:\r\n              path: /probe/live\r\n              port: 8080\r\n            initialDelaySeconds: 45\r\n            periodSeconds: 1\r\n            failureThreshold: 10","basefibonacci-web-serviceyml#base/fibonacci-web-service,yml":"apiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: fibonacci-backend-web-service\r\n  namespace: fibonacci\r\nspec:\r\n  selector:\r\n    app: fibonacci-backend-web\r\n  ports:\r\n    - protocol: TCP\r\n      port: 8080","basefibonacci-web-hpayml#base/fibonacci-web-hpa.yml":"apiVersion: autoscaling/v2\r\nkind: HorizontalPodAutoscaler\r\nmetadata:\r\n  name: fibonacci-backend-hpa\r\n  namespace: fibonacci\r\nspec:\r\n  minReplicas: 2\r\n  maxReplicas: 5\r\n  scaleTargetRef:\r\n    apiVersion: apps/v1\r\n    kind: Deployment\r\n    name: fibonacci-backend-web\r\n  metrics:\r\n    - type: Resource\r\n      resource:\r\n        name: cpu\r\n        target:\r\n          type: Utilization\r\n          averageUtilization: 50\r\n  behavior:\r\n    scaleUp:\r\n      stabilizationWindowSeconds: 30\r\n    scaleDown:\r\n      stabilizationWindowSeconds: 30","basefibonacci-web-namespaceyml#base/fibonacci-web-namespace.yml":"apiVersion: v1\r\nkind: Namespace\r\nmetadata:\r\n  name: fibonacci-web","overlay#overlay":"overlay 에 정의하는 kustomization.yml 에는 주로 배포 Phase 에 맞는 환경, 이미지 명, 태그, 볼륨 명, 각종 환경 변수 등을 덮어쓰는 내용 들을 정의합니다.","overlaydevelopkustomizationyml#overlay/develop/kustomization.yml":"apiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nresources:\r\n  - ../../base\r\nimages:\r\n  - name: chagchagchag/fibonacci-backend-web\r\n  - newName: chagchagchag/fibonacci-backend-web\r\n  - newTag: 0.0.1-fibonacci-backend-web.01\r\nnamespace: fibonacci-web-develop","overlayproductionkustomizationyml#overlay/production/kustomization.yml":"apiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nresources:\r\n  - ../../base\r\nimages:\r\n  - name: chagchagchag/fibonacci-backend-web\r\n  - newName: chagchagchag/fibonacci-backend-web\r\n  - newTag: 0.0.1-fibonacci-backend-web.01\r\nnamespace: fibonacci-web-production"}},"/fibonacci-backend-web/backend-web-jib-build":{"title":"Backend Web Jib Build","data":{"gradle-jib-빌드-정의-이미지-생성-푸시#Gradle Jib 빌드 정의, 이미지 생성, 푸시":"프로젝트 Github\nbuild.gradle\n도커이미지를 빌드,푸시하는 방식에 대해 정리합니다. Gradle Jib 빌드 방식에 대해서 정리하지만, Dockerfile 로 빌드하는 방식에 대해서도 문서의 마지막에 추가로 정리해두었습니다.","참고#참고":"github.com/jib/jib-gradle-plugin\ngithub.com/GoogleContainerTools/jib","javakotlin-애플리케이션의-도커이미지-빌드-방식#Java/Kotlin 애플리케이션의 도커이미지 빌드 방식":"Java/Kotlin 애플리케이션의 Gradle 빌드 시 두가지를 선택할 수 있습니다\nGradle Jib 플러그인을 사용하는 빌드 & 푸시\nDockerfile 정의, Shell Script 를 이용한 빌드 & 푸시\n두 방법 중에는 일반적으로 Gradle Jib 을 이용한 방식이 많이 사용되는 편입니다.","gradle-jib-으로-docker-이미지-빌드--푸시#Gradle Jib 으로 Docker 이미지 빌드 & 푸시":"","plugin-추가#plugin 추가":"plugins {\r\n    // ...\r\n\tid 'com.google.cloud.tools.jib' version '3.4.0'\r\n}\n코틀린 DSL의 build.gradle.kts 파일에서는 아래와 같이 작성합니다.\nplugins {\r\n  // ...\r\n  id(\"com.google.cloud.tools.jib\") version \"3.4.0\"\r\n  // ...\r\n}","빌드스크립트-작성#빌드스크립트 작성":"jib {\r\n\tfrom {\r\n\t\timage = \"amazoncorretto:17\"\r\n\t}\r\n\r\n\tto {\r\n\t\timage = \"chagchagchag/fibonacci-backend-web\"\r\n\t\ttags = [\"0.0.1\", \"0.0.1.fibonacci-backend-web.01\", \"latest\"]\r\n\t}\r\n\r\n\tcontainer {\r\n\t\tcreationTime = \"USE_CURRENT_TIMESTAMP\"\r\n\t}\r\n}\n위의 빌드스크립트의 경우 코틀린 문법과 다른 부분이 없기에 코틀린 DSL 의 build.gradle.kts 파일을 작성시에도 위의 내용을 그대로 사용하면 됩니다.젠킨스나 Github CI에서 빌드하는 것이 아닌 경우 개발자의 PC에 따라 CPU 가 달라지는 것으로 인해 애플 M1 등 여러가지 빌드 옵션을 직접 추가하거나 이런 작업들이 필요한 경우가 있습니다.이런 경우 jib 내의 from 구문에 아래와 같은 내용을 작성해주시면 됩니다.\n// ...\r\n\r\njib {\r\n    from {\r\n        image = \"amazoncorretto:17\"\r\n\r\n        platforms {\r\n            platform{\r\n                architecture = \"arm64\"\r\n                os = \"linux\"\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n// ...\n회사에서 실제 개발 작 시 필요한 내용들을 정의할 때 세부적인 내용들이 많이 필요할 수 있는데 자세한 설명은 github.com/jib/jib-gradle-plugin 에서 확인할 수 있습니다.","빌드--push#빌드 & Push":"이렇게 작성한 build 스크립트는 gradle 명령어로 빌드할 수도 있고, intellij 에서 jib 태스크를 실행해서 도커이미지를 빌드 후 푸시할 수도 있습니다.참고로 개발 PC에서 빌드 실행 시 Docker Desktop 이 실행 중이어야 합니다.","intellij-내에서-jib-태스크-실행#intellij 내에서 jib 태스크 실행":"인텔리제이 내에서는 아래와 같이 jib 태스크 버튼을 눌러서 태스크를 실행해주시면 됩니다.","gradlew-명령어로-실행#gradlew 명령어로 실행":"gradlew 명령어로 실행하는 것은 아래와 같이 하면 됩니다.\n./gradlew fibonacci-backend-web:jib","컨테이너-이미지-동작-확인#컨테이너 이미지 동작 확인":"도커 이미지 pull (만약 로컬에 이미지가 없다면)\n$ docker pull chagchagchag/fibonacci-backend-web:0.0.1\n도커 이미지 구동\ndocker container run --rm -d -p 8080:8080 --name fibonacci-backend-web-local chagchagchag/fibonacci-backend-web:0.0.1\r\n\r\n동작 확인\n$ curl http://localhost:8080/probe/healthcheck\r\nOK\n종료\ndocker container stop fibonacci-backend-web-local\r\n\r\nfibonacci-backend-web-local","dockerfile-로-docker-이미지-빌드--푸시#Dockerfile 로 Docker 이미지 빌드 & 푸시":"","dockerfile-정의-빌드--푸시#Dockerfile 정의, 빌드 & 푸시":"Dockerfile_simple\nFROM amazoncorretto:17\r\nWORKDIR deploy\r\nCOPY build/libs/fibonacci_backend_web-0.0.1.jar app.jar\r\nENTRYPOINT [\"java\", \"-jar\", \"app.jar\"]\n이미지 빌드\ndocker build -f Dockerfile_simple --tag chagchagchag/fibonacci-backend-web-simple:0.0.1 .\n컨테이너 구동\n$ docker container run --name fibonacci-backend-web-simple --rm -d -p 8080:8080 chagchagchag/fibonacci-backend-web-simple:0.0.1\r\n\r\n\r\n## 확인\r\n$ docker container ls\r\nCONTAINER ID   IMAGE                                               COMMAND               CREATED         STATUS\r\nPORTS                    NAMES\r\ncf160b8e1ae2   chagchagchag/fibonacci-backend-web-simple:0.0.1   \"java -jar app.jar\"   6 seconds ago   Up 4 seconds   0.0.0.0:8080->8080/tcp   fibonacci-backend-web-simple\nAPI 확인\n$ curl http://localhost:8080/probe/healthcheck\r\nOK\n종료&삭제\n$ docker container stop fibonacci-backend-web-simple\r\nfibonacci-backend-web-simple","이미지-리포지터리#이미지 리포지터리":"예전의 레거시 개발 방식에서는 Docker 이미지라는 것이 필요가 없었습니다. 현재는 Docker Container 방식의 이미지를 구동하는 방식이 대중화되어 있습니다. 이때 이미지 리포지터리에 개발버전이 잘못 Push 될 수 있는 경우가 있습니다.이런 이유로 가급적이면 개발 용도의 이미지 리포지터리와 Deploy (배포) 전용 이미지 리포지터리를 따로 운영해서 Github Action 등에서만 사용하는 Deploy 전용 이미지 리포지터리를 사용하는 것을 권장드립니다.\n보통 이미지 리포지터리는 S3 방식으로 운영되는 것으로 알고 있습니다. 따라서 AWS에서 ECR 을 운영,개발 용도로 분리해서 사용한다고 하더라도 용량에 큰 부담도 없고 비용이 그렇게 크게 발생하지는 않을 듯 해보입니다."}},"/fibonacci-backend-web/introduce-fibonacci-backend-web":{"title":"Introduce Fibonacci Backend Web","data":{"소개#소개":""}},"/fibonacci-backend-web/backend-web-perf-test":{"title":"Backend Web Perf Test","data":{"부하테스트#부하테스트":""}},"/fibonacci-backend-web/backend-web-k8s-yaml":{"title":"Backend Web K8s Yaml","data":{"k8s-리소스-정의--로컬-k8s-확인#k8s 리소스 정의 & 로컬 k8s 확인":"fibonacci-backend-web 에 비해 단순한 구조로 작성했습니다.","들어가기-전에#들어가기 전에..":"이번 페이지는 ArgoCD나 이런 것들을 설치하지 않은 간단한 버전의 클러스터를 기준으로 리소스들을 kubectl 로 하나 하나 만들어가면서 테스트해가면서 기능들을 완성하는 과정을 설명하기 위한 것이 목적입니다.이번 backend 예제를 구동할 kind 클러스터 정의는 cluster/single-cluster.yml 에 있고, 쉘스크립트는 cluster/create-single-cluster.sh 입니다.\ncluster/single-cluster.yml\ncluster/create-single-cluster.sh","namespace-정의#namespace 정의":"namespace가 존재하지 않는다면 예제 테스트를 위한 namespace를 정의합니다. namespace 는 fibonacci 입니다.\n$ kubectl create ns fibonacci\r\nnamespace/fibonacci created\n또는 아래의 namespace 파일을 적용해줍니다.\napiVersion: v1\r\nkind: Namespace\r\nmetadata:\r\n  name: fibonacci\n$ kubectl apply -f fibonacci-web-namespace.yml","deployment-정의#Deployment 정의":"Deployment 의 내용은 아래와 같습니다. 자세한 설명은 추후 추가하도록 하겠습니다.\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: fibonacci-backend-web-deploy\r\n  namespace: fibonacci\r\nspec:\r\n  replicas: 2\r\n  strategy:\r\n    type: RollingUpdate\r\n    rollingUpdate:\r\n      maxSurge: 1\r\n      maxUnavailable: 0\r\n  selector:\r\n    matchLabels:\r\n      app: fibonacci-backend-web\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: fibonacci-backend-web\r\n    spec:\r\n      terminationGracePeriodSeconds: 60\r\n      containers:\r\n        - name: fibonacci\r\n          image: chagchagchag/fibonacci-backend-web:0.0.1\r\n          imagePullPolicy: Always\r\n          resources:\r\n            requests:\r\n              memory: \"512Mi\"\r\n              cpu: \"1000m\"\r\n            limits:\r\n              memory: \"1Gi\"\r\n              cpu: \"1500m\"\r\n          lifecycle:\r\n            preStop:\r\n              exec:\r\n                command: [\"/bin/sh\",\"-c\",\"sleep 10\"]\r\n          startupProbe:\r\n            httpGet:\r\n              path: /probe/startup\r\n              port: 8080\r\n            initialDelaySeconds: 45\r\n            periodSeconds: 1\r\n            failureThreshold: 10\r\n          readinessProbe:\r\n            httpGet:\r\n              path: /probe/ready\r\n              port: 8080\r\n            initialDelaySeconds: 45\r\n            periodSeconds: 1\r\n            successThreshold: 1\r\n            failureThreshold: 2\r\n          livenessProbe:\r\n            httpGet:\r\n              path: /probe/live\r\n              port: 8080\r\n            initialDelaySeconds: 45\r\n            periodSeconds: 1\r\n            failureThreshold: 10","service-정의#Service 정의":"Service 정의 입니다. 클러스터 내부에서 8080 포트에 대해 fibonacc-backend-web Deployment를 찾아서 연결해줍니다. 별도의 Service Type 을 명시하지 않았기에 Cluster IP 타입의 서비스 타입으로 명시되었음을 확인 가능합니다.\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: fibonacci-backend-web-service\r\n  namespace: fibonacci\r\nspec:\r\n  selector:\r\n    app: fibonacci-backend-web\r\n  ports:\r\n    - protocol: TCP\r\n      port: 8080","hpa-정의#HPA 정의":"HPA 는 Horizontal Pot Autoscaling 의 약자이며, 오토스케일링과 관련된 쿠버네티스 리소스입니다.\n쿠버네티스 문서 - Horizontal Pod Autoscaling\n자세한 설명은 별도의 섹션에서 개념설명을 요약할 예정이고, 이번 문서에서는 리소스 정의 파일만을 간단하게 남겨두고 넘어가기로 했습니다.\napiVersion: autoscaling/v2\r\nkind: HorizontalPodAutoscaler\r\nmetadata:\r\n  name: fibonacci-backend-hpa\r\n  namespace: fibonacci\r\nspec:\r\n  minReplicas: 2\r\n  maxReplicas: 5\r\n  scaleTargetRef:\r\n    apiVersion: apps/v1\r\n    kind: Deployment\r\n    name: fibonacci-backend-web\r\n  metrics:\r\n    - type: Resource\r\n      resource:\r\n        name: cpu\r\n        target:\r\n          type: Utilization\r\n          averageUtilization: 50\r\n  behavior:\r\n    scaleUp:\r\n      stabilizationWindowSeconds: 30\r\n    scaleDown:\r\n      stabilizationWindowSeconds: 30","통합된-리소스-yaml-파일#통합된 리소스 yaml 파일":"kustomize 를 통해서 조금 더 편하게 할수도 있겠지만, 이번 실습에서는 하나의 yaml 파일에 합쳐둔 통합 yaml 파일을 적용하는 과정을 살펴봅니다.통합된 yaml 파일의 내용은 아래와 같습니다.\napiVersion: v1\r\nkind: Namespace\r\nmetadata:\r\n  name: fibonacci\r\n---\r\napiVersion: autoscaling/v2\r\nkind: HorizontalPodAutoscaler\r\nmetadata:\r\n  name: fibonacci-backend-hpa\r\n  namespace: fibonacci\r\nspec:\r\n  minReplicas: 2\r\n  maxReplicas: 5\r\n  scaleTargetRef:\r\n    apiVersion: apps/v1\r\n    kind: Deployment\r\n    name: fibonacci-backend-web\r\n  metrics:\r\n    - type: Resource\r\n      resource:\r\n        name: cpu\r\n        target:\r\n          type: Utilization\r\n          averageUtilization: 50\r\n  behavior:\r\n    scaleUp:\r\n      stabilizationWindowSeconds: 30\r\n    scaleDown:\r\n      stabilizationWindowSeconds: 30\r\n---\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: fibonacci-backend-web-service\r\n  namespace: fibonacci\r\nspec:\r\n  selector:\r\n    app: fibonacci-backend-web\r\n  ports:\r\n    - protocol: TCP\r\n      port: 8080\r\n---\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: fibonacci-backend-web-deploy\r\n  namespace: fibonacci\r\nspec:\r\n  replicas: 2\r\n  strategy:\r\n    type: RollingUpdate\r\n    rollingUpdate:\r\n      maxSurge: 1\r\n      maxUnavailable: 0\r\n  selector:\r\n    matchLabels:\r\n      app: fibonacci-backend-web\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: fibonacci-backend-web\r\n    spec:\r\n      terminationGracePeriodSeconds: 60\r\n      containers:\r\n        - name: fibonacci\r\n          image: chagchagchag/fibonacci-backend-web:0.0.1\r\n          imagePullPolicy: Always\r\n          resources:\r\n            requests:\r\n              memory: \"512Mi\"\r\n              cpu: \"1000m\"\r\n            limits:\r\n              memory: \"1Gi\"\r\n              cpu: \"1500m\"\r\n          lifecycle:\r\n            preStop:\r\n              exec:\r\n                command: [\"/bin/sh\",\"-c\",\"sleep 10\"]\r\n          startupProbe:\r\n            httpGet:\r\n              path: /probe/startup\r\n              port: 8080\r\n            initialDelaySeconds: 45\r\n            periodSeconds: 1\r\n            failureThreshold: 10\r\n          readinessProbe:\r\n            httpGet:\r\n              path: /probe/ready\r\n              port: 8080\r\n            initialDelaySeconds: 45\r\n            periodSeconds: 1\r\n            successThreshold: 1\r\n            failureThreshold: 2\r\n          livenessProbe:\r\n            httpGet:\r\n              path: /probe/live\r\n              port: 8080\r\n            initialDelaySeconds: 45\r\n            periodSeconds: 1\r\n            failureThreshold: 10"}},"/":{"title":"Introduction","data":{"":"fibonacci-backend 프로젝트 문서 페이지\nGithub : github.com/chagchagchag/fibonacci-backend\n프로젝트 문서 : chagchagchag.github.io/docs-fibonacci-backend\nSpring Boot 기반의 k8s 애플리케이션을 어떻게 개발해나가는지 매뉴얼을 정리"}},"/kubernetes-overview/PV-PVC":{"title":"Pv Pvc","data":{"pv-pvc#PV, PVC":"","참고자료#참고자료":"Kubernetes - Volumes\n핵심만 콕! 쿠버네티스","pv-pvc-1#PV, PVC":"쿠버네티스에서는 데이터 저장소를 프로비저닝 쿠버네티스에서는 데이터 저장소를 프로비저닝(Provisioning(제공))하는 부분과 저장소를 사용하는 부분으로 분류합니다. PV는 프로비저닝을 담당하고, PVC는 저장소를 사용하는 역할을 합니다.\r\n즉, PV를 하나의 볼륨으로만 인식할 뿐이고, PVC를 거쳐서 PV에 Read/Write 를 수행합니다.\r\nPV 로 특정 볼륨을 마운트한 컨테이너는 새로운 노드에서 컨테이너/파드가 새로 기동되더라도 PV 볼륨에 저장된 데이터를 이어서 계속 사용가능합니다.\nPV 는 hostPath 기반으로 선언할 수도 있고, NFS 기반으로 선언할 수도 있고, AWSElasticBlockStore PV 로도 선언할 수 있습니다.\r\n이 외에도 다양한 방식으로 사용될 수 있는데 아래와 같은 다양한 방벋들이 있습니다.\nazureDisk : Azure 클라우드 플랫폼에서 제공하는 볼륨 서비스\nemptyDir : Pod 레벨에서의 볼륨개념이다. 주로 같은 Pod 내 컨테이너 들 끼리 filesystem을 통한 정보를 주고받을때 많이 사용한다.\ndownward API\nconfigMap\nAWS 클라우드 플랫폼 기반이라면 가장 선호되는 방식은 AWSElasticBlockStore 방식이겠지만, 경우에 따라 취사선택을 하는 경우가 많습니다.\r\n클라우드에서 제공하는 볼륨 서비스로는 GCP플랫폼에서는 PersistentDisk, AWS플랫폼에서는 EBS(Elastic Bock Storage) 라는 볼륨을 사용하는 방식이 있습니다.\nPVC 는 Persistent Volume Claim 의 약자로 PV 리소스를 요청하는 역할을 수행합니다. 클러스터 관리자가 PersistentVolume 을 통해 데이터 저장소를 준비하면 쿠버네티스 사용자(애플리케이션 개발자)가 PVC 요청을 통해 해당 리소스를 선점합니다.","pv-pvc-리소스-선언#PV, PVC 리소스 선언":"PV\nkind 를 PersistentVolume 으로 선언하고 세부적인 spec 내에 용량은 어느 정도로 할지 액세스 모드는 어떻게 할 것인지를 명시해서 하나의 리소스로 정의합니다.\nspec.capacity.storage 에 지정하는 저장 단위는 Gi, Mi 가 있다.\nPVC\nspec.storageClassName 에 어떤 종류의 저장소 타입을 지정할지를 지정합니다.\nspec.resources[i].requests.storage 에 어느 정도의 크기의 저장소를 지정할지를 명시합니다.\nspec.accessModes 에는 접근 모드를 설정합니다. NFS 를 사용할 경우 ReadWirteMany를 사용하기도 합니다.","hostpath-pv-pvc-예제#hostPath PV, PVC 예제":"apiVersion: v1\r\nkind: PersistentVolume\r\nmetadata:\r\n  name: k8shelloboot-pv-hostpath\r\nspec:\r\n  storageClassName: manual # 저장소 타입 지정\r\n  capacity: # 저장소 크기 \r\n    storage: 500Mi\r\n  accessModes: # 접근 모드 \r\n    - ReadWriteOnce # 동시에 1개의 파드만 볼륨에 접근할 수 있다\r\n  hostPath:\r\n    path: /run/desktop/mnt/host/v/000.env/volume\n이렇게 만든 PV 는 아래와 같이 확인 가능합니다.\n$ kubectl get pv\r\nNAME                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE\r\nk8shelloboot-pv-hostpath   500Mi      RWO            Retain           Available           manual                  4s\n이번에는 PVC 를 생성합니다.\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: k8shelloboot-pvc-hostpath\r\nspec:\r\n  storageClassName: manual\r\n  resources:\r\n    requests:\r\n      storage: 500Mi\r\n  accessModes:\r\n    - ReadWriteOnce\n생성한 PVC 를 적용해봅니다.\n$ kubectl apply -f k8shelloboot-pvc-hostpath.yml\r\npersistentvolumeclaim/k8shelloboot-pvc-hostpath created\n이번에는 PV가 잘 연결되었는지 확인해봅니다.\n$ kubectl get pv\r\nNAME                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                               STORAGECLASS   REASON   AGE\r\nk8shelloboot-pv-hostpath   500Mi      RWO            Retain           Bound    default/k8shelloboot-pvc-hostpath   manual                  3m1s\nPVC 연결 전에는 STATUS 가 Available 였는데 연결 후에는 STATUS가 Bound 로 변한 것을 확인 가능합니다.","nfs-pv-pvc-예제#NFS PV, PVC 예제":"nfs 는 StorageClass 와 nfs-server-provisioner 를 이용한 실습이 가능한데, 이 것에 대해서는 나중에 정리해둘 예정이고, 일단은 PV 리소스 정의를 어떻게 하는지만 정리해두었습니다.\napiVersion: v1\r\nkind: PersistentVolume\r\nmetadata:\r\n  name: k8shello-pv-nfs\r\nspec:\r\n  storageClassName: nfs # 저장소 타입 지정\r\n  capacity: # 저장소 크기 지정\r\n    storage: 500Mi\r\n  accessModes: # 접근 모드 \r\n    - ReadWriteMany # nfs 는 ReadWriteMany 로 여러 Pod 에서 접근이 가능 \r\n  mountOptions:\r\n    - hard\r\n    - nfsvers=4.1\r\n  nfs:\r\n    path: /tmp/nfs\r\n    server: <NFS 서버 IP>","ebs-pv-예제#EBS PV 예제":"apiVersion: v1\r\nkind: PersistentVolume\r\nmetadata:\r\n  name: k8shelloboot-pv-ebs\r\nspec:\r\n  capacity:\r\n    storage: 1Gi # 데이터 저장소 크기 \r\n  accessModes: # 접근모드\r\n    - ReadWriteOnce\r\n  awsElasticBlockStore: # AWS EBS 정보 입력\r\n    volumeID: <Volume ID>\r\n    fsType: ext4"}},"/kubernetes-overview/hostPath-emptyDir":{"title":"hostPath, emptyDir","data":{"":"hostPath는 노드(호스트)의 디렉터리를 파드에 마운트하는 방식이고, emptyDir 은 노드(호스트)의 디스크를 컨테이너의 볼륨으로 마운트하는 방식이다.\r\nhostPath 는 파드가 재시작되더라도 노드(호스트)에 데이터가 남아있다. 다만 파드가 다른 노드에서 재시작 되면 이전에 사용했던 데이터를 이용하는 것은 불가능하다.\r\nemptyDir 은 파드가 살아있는 동안 컨테이너가 장애 등으로 인해 재기동되더라도 컨테이너는 파드가 마운트하고 있는 emptyDir 을 그대로 이어서 사용 가능하다. 다만 파드가 소멸되면 emptyDir 볼륨 내의 데이터도 소멸된다.\nemptyDir 은 Pod 레벨의 생명주기를 가진 임시저장소다. hostPath 는 호스트(노드) 레벨에서의 볼륨이다.\r\nhostPath는 노드 레벨, emptyDir 은 파드 레벨에서의 지역적인 볼륨의 개념이라고 생각한다면 이해가 쉽다.\r\n즉 hostPath, emptyDir 은 하나의 노드 또는 하나의 파드 내에서만의 볼륨을 사용하는 개념이다.뒤에서 따로 정리할 PV/PVC 문서에서는 여러 노드가 공유할 수 있는 볼륨을 쿠버네티스의 PV 리소스로 선언하고 PVC를 통해 접근하는 예제와 개념들을 살펴본다.\r\nhostPath, emptyDir 모두 PV 로 선언할 수 있기는 하다.\r\n하지만, NFS, AWSElasticBlockStore 기반의 볼륨으로 사용하는 것이 더 권장되는 방식이다.","예제#예제":"Dockerhub - chagchagchag/k8shelloboot\ngithub/chagchagchag/eks-k8s-docker-study-archive/application-examples/k8shello\nk8s/k8shelloboot-hostpath.yml\nk8s/k8shelloboot-emptydir.yml","hostpath#hostPath":"hostPath 는 노드(호스트 머신)의 디렉터리를 파드에 마운트하는 방식이다. 위에서 살펴본 emptyDir 은 호스트의 디스크를 컨테이너의 볼륨으로 마운트하지만, hostPath 는 호스트의 디스크를 파드에 마운트한다는 사실에 유의하자.\n“파드가 Host 의 Path 를 마운트한다.” 라고 생각한다면 이해가 쉽다.\nhostPath 로 볼륨을 마운트하면 파드가 재시작 되더라도 호스트에 데이터가 남아있다. 다만 파드가 새로운 호스트(노드)에서 재시작될 경우는 새로운 호스트(노드)의 디렉터리를 사용하므로, 이전 노드에서 사용하던 데이터는 접근불가능하다.\nhostPath 는 흔히 Host Volume 이라고 부르기도 한다. 도커의 -v 옵션과 유사하게 host 서버의 볼륨 공간에 Pod가 데이터를 저장하는 것을 의미한다.\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: k8shelloboot-app-hostpath\r\nspec:\r\n  containers:\r\n  - name: k8shelloboot-app-hostpath\r\n    image: chagchagchag/k8shelloboot:0.0.1\r\n\r\n    # 컨테이너 내부의 연결 위치 지정\r\n    volumeMounts:\r\n    - mountPath: /app/volume\r\n      name: v-volume\r\n\r\n  # host 서버의 연결 위치 지정\r\n  volumes:\r\n  - name: v-volume\r\n    hostPath:\r\n      path: /run/desktop/mnt/host/v/000.env/volume\r\n      # type: DirectoryOrCreate\nvolumeMounts : 컨테이너 내부에서 사용될 볼륨을 선언\nmountPath: 컨테이너 내부에 볼륨이 연결될 위치를 지정. 컨테이너 내부의 운영체제 내의 /app/volume 디렉터리에 볼륨이 마운트된다.\nname: volumeMounts와 volume 을 연결하는 식별자로 사용된다. 위의 예제에서는 v-volume 이다.\nvolumes : Pod 에서 사용할 volume 을 지정\nname: volumeMounts 와 volume 을 연결하는 식별자로 사용. 위의 예제에서는 v-volume 이다.\nhostPath : 호스트 서버 내에 연결 위치를 지정. 위의 예제에서는 /run/desktop/mnt/host/v/000.env/volume 이다.","실행--확인#실행 & 확인":"$ kubectl apply -f k8shelloboot-hostpath.yml\r\npod/k8shelloboot-app-hostpath created\r\n\r\n$ kubectl exec -it k8shelloboot-app-hostpath -- bash\r\nbash-4.2#\r\nbash-4.2# curl http://localhost:8080/healthcheck\r\nOK\r\n# 정상적으로 출력됨을 확인 가능\r\n\r\nbash-4.2# cat /app/volume/cache-log.log\r\n2024-01-30T11:27:27.320095147>>> write OK\r\n\r\nbash-4.2# exit\r\nexit\r\n\r\n## 삭제\r\n$ kubectl delete -f k8shelloboot-hostpath.yml\r\npod \"k8shelloboot-app-hostpath\" deleted\r\n\r\n## 재생성\r\n$ kubectl apply -f k8shelloboot-hostpath.yml\r\npod/k8shelloboot-app-hostpath created\r\n\r\n## bash 접속\r\n$ kubectl exec -it k8shelloboot-app-hostpath -- bash\r\nbash-4.2#\r\n\r\n## API 호출\r\nbash-4.2# curl http://localhost:8080/healthcheck\r\nOK\r\n\r\nbash-4.2# cat /app/volume/cache-log.log\r\n2024-01-30T11:27:27.320095147>>> write OK\r\n2024-01-30T11:27:34.827718406>>> write OK\r\nbash-4.2#","emptydir#emptyDir":"emptyDir 은 노드(호스트 머신)의 디스크를 컨테이너의 볼륨으로 마운트해서 사용하는 방식이다.\n파드가 소멸될 때 emptyDir 에 할당했던 볼륨 내의 데이터도 소멸된다.\n파드가 살아있는 동안은 컨테이너가 장애 등으로 인해 재기동 되더라도 컨테이너는 파드가 마운트하고 있는 emptyDir 을 그대로 사용 가능하다. 다만, 위에서도 이야기했듯 파드가 소멸되면 emptyDir 볼륨 내의 데이터도 소멸된다.\n리소스 정의 파일 작성 시 볼륨을 선언하는 구문과 컨테이너에서 볼륨을 마운트하는 구문을 따로 정의한다는 사실을 기억해야 한다.\n위에서 정리한 hostPath 는 노드(호스트) 레벨이라면, emptyDir은 파드레벨이라는 점을 떠올리면 이해가 쉽다.","예제-1#예제":"apiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: k8shelloboot-app-emptydir\r\nspec:\r\n  containers:\r\n  - name: k8shelloboot-app-emptydir\r\n    image: chagchagchag/k8shelloboot:0.0.1\r\n\r\n    # 컨테이너 내부의 연결 위치 지정\r\n    volumeMounts:\r\n    - mountPath: /app/volume\r\n      name: v-volume\r\n\r\n  # host 서버의 연결 위치 지정\r\n  volumes:\r\n  - name: v-volume\r\n    emptyDir: {}","실행--확인-1#실행 & 확인":"$ kubectl apply -f k8shelloboot-emptyDir.yml\r\npod/k8shelloboot-app-emptydir created\r\n\r\n$ kubectl get all\r\nNAME                            READY   STATUS    RESTARTS   AGE\r\npod/k8shelloboot-app-emptydir   1/1     Running   0          11s\r\n\r\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\r\nservice/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   13h\r\n\r\n$ kubectl exec -it k8shelloboot-app-emptydir -- bash\r\n\r\nbash-4.2# curl http://localhost:8080/healthcheck\r\nOK\r\n\r\nbash-4.2# cat /app/volume/cache-log.log\r\n2024-01-31T00:53:34.497769035>>> write OK\r\n\r\n\r\nbash-4.2# curl http://localhost:8080/healthcheck\r\nOK\r\n\r\nbash-4.2# cat /app/volume/cache-log.log\r\n2024-01-31T00:53:34.497769035>>> write OK\r\n2024-01-31T00:54:05.518777545>>> write OK\r\n\r\nbash-4.2# exit\r\nexit\r\n\r\n$ kubectl delete -f k8shelloboot-emptyDir.yml\r\npod \"k8shelloboot-app-emptydir\" deleted\r\n\r\n$ kubectl apply -f k8shelloboot-emptyDir.yml\r\npod/k8shelloboot-app-emptydir created\r\n\r\n$ kubectl exec -it k8shelloboot-app-emptydir -- bash\r\nbash-4.2#\r\n\r\n\r\nbash-4.2# cat /app/volume/cache-log.log\r\ncat: /app/volume/cache-log.log: No such file or directory\r\n\r\n\r\nbash-4.2# curl http://localhost:8080/healthcheck\r\nOK\r\n\r\nbash-4.2# cat /app/volume/cache-log.log\r\n2024-01-31T00:56:46.765727615>>> write OK\r\n\r\n\r\nbash-4.2# exit\r\nexit"}},"/kubernetes-overview/intro":{"title":"Intro","data":{"소개#소개":"현재 정리중이지만 급하게 추가 중입니다. 일단 시간이 나는대로 빠르게 정리 중이라 시간이 부족해서 존댓말로 적지는 않았습니다.\r\n시간이 된다면 존대말로 변경하도록 하겠습니다."}},"/kubernetes-overview/kubernetes-resource-assignment":{"title":"Kubernetes Resource Assignment","data":{"kubernetes-pod-의-자원-설정#Kubernetes Pod 의 자원 설정":"애플리케이션의 구조, 동작, 프레임워크에 따라 달라지기 때문에 모니터링을 통해 측정을 한 후에 자원을 설정하는 경우가 많다.또는 애플리케이션에 대해서 성능 테스트를 수행해본다던가 실제로 운영을 하면서 자원사용량 들을 모니터링을 하면서 CPU가 부족한지 메모리가 여유있는지, 힙메모리가 부족한지, 가비지 컬렉션 등이 발생하는지 이런 것들을 보면서 판단을 내리는 과정 역시 중요하다. Pod의 수량에 따라서 CPU, Memory 사용량 역시 달라질 수 있는데 파드가 많아지면 CPU 사용량은 줄고, 메모리 사용량은 늘어나고 파드가 적어지면 CPU 사용량은 늘고 메모리 사용량은 줄어들고 하는 이런 내용들이 달라질 수 있다.Pod 하나에 대해서 테스트와 모니터링을 해보면서 Pod가 요구하는 CPU, Memory 최소 요구값, 최대 상한선을 미리 정한다면 이 것을 바탕으로 적정한 자원 할당량을 정할 수 있고 파드 수량을 정할 수도 있다.주의할 점이 있다. Pod 의 수량을 늘린다고 해서 무조건 성능을 선형적으로 올려주지는 않는 다는 사실을 이해해야 한다.1기의 Pod로 운영되던 것을 10개로 늘린다고 해서 처리량 까지 10배로 늘어나는 것은 아니다. 병목구간이 얼마나 적은지 애플리케이션이 얼마나 독립적으로 요청을 처리할 수 있는지에 따라 파드의 수량을 결정하는 것이 현명하다.Stateless 한 애플리케이션 이면서 데이터베이스 처리량이 높다면 보통은 스케일 아웃을 통해서 성능 향상을 하는 것이 효율을 보인다. 그렇지 않을 경우에는 스케일아웃을 통한 성능 향상은 효과를 크게 보지 못한다. 이런 경우 파드의 수량을 두배, 세배까지 올려도 성능향상을 얻지 못하는 경우가 많다. 트래픽이 높다고 해서 애플리케이션의 파드 수량을 100배, 1000배 까지 올린다고해서 성능향상을 확실하게 얻을 수 있는 것은 아니다.애플리케이션의 성격, 구조에 따라서 스케일아웃, 스케일업을 할지에 대한 관점도 달라지게 된다. 예를 들어 Spring MVC 의 경우 스레드를 많이 사용하는데 CPU 자원이 많지 않으면 제대로 성능이 나오기 쉽지 않다. Spring Boot 의 경우 스레드를 200개 까지 만들어내는데 일반적으로 요청들이 DB 작업 등으로 인해 블로킹 되어서 대기하는 그런 시간들이 많기 때문에 일반적으로 스프링 MVC 의 경우에는 스레드를 많이 만들고 많이 유지하는 경우가 많다. 즉, CPU 자원 자체를 1core 미만으로 잡으면 좋은 성능을 내기 쉽지 않다. 이런 경우에는 CPU 자체를 늘리는 것이 파드의 수량을 늘리는 것보다 중요하다고 할 수 있다. 반면, 스레드를 사용하지 않고 이벤트 루프를 이용해서 처리하는 비동기 서버들이 최근 많아지는 추세인데 이런 파드들의 경우에는 파드를 스케일 업 해주기보다는 CPU를 1 core 이하로 할당한 다수의 파드를 운영하는 스케일 아웃 형태가 적절하다.마지막으로 노드의 자원을 여유를 두고 사용하는 것 역시 중요하다. 노드 자원을 모두 사용하게끔 타이트하게 파드의 스케일 아웃을 스펙으로 잡아두었다면 파드의 업데이트, 스케일 아웃 시에 파드가 자주 Pending 상태에 걸리게 된다. 이렇게 되면 쿠버네티스의 빠른 확장, 빠른 배포라는 장점을 활용하지 못하게 된다.노드에 여유가 없어서 새로운 노드를 추가하는 과정에서는 노드를 추가하는 데에 시간이 상당 시간 소요되는 편이고(파드 생성에 비해 노드 생성은 큰 작업이므로) 노드가 새로 추가되었을 때 새로운 노드로 파드 들이 몰린다거나 하는 현상이 발생할 수 있어서 이런 부분들이 단점이 될 수 있다. 따라서 트래픽이 급격하게 증가했을 때에도 대응이 가능한 수준을 노드의 사양을 설정하는 것 역시 중요하다.","resources--pod-의-resource#resources : Pod 의 Resource":"requests, limits 는 효과적인 스케일링이 가능하도록 하게 하려면 가급적 정의를 꼭 해주는 것이 좋습니다.\nresources:\r\n  requests: ## 1) \r\n    memory: \"512Mi\"\r\n    cpu: \"250m\"\r\n  limits:  ## 2)\r\n    memory: \"1Gi\"\r\n    cpu: \"500m\"","1-requests--최소로-필요한-기본적인-자원의-크기-등을-명시#1) requests : 최소로 필요한 기본적인 자원의 크기 등을 명시":"requests 는 주로 파드(Pod)가 노드 내에 배포될 때 노드 내에서 파드 하나가 확보해야 하는 자원의 크기를 의미한다.\n파드가 노드 내에 배포될 때 노드 내에서 \"파드 하나에 대해 최소 이 정도는 있어야 한다\"를 의미\n가급적 애플리케이션이 안정적으로 돌아갈 수 있는 최소한도의 자원의 크기로 설정해줘야 한다.\n만약 노드가 requests 에 해당하는 자원만큼을 할당할 만한 여력이 없다면 Pod 자체를 스케쥴링하지 않고 Pending 상태로 둔 후 자원이 남는 노드가 생길 때 까지 대기한다.\nrequests 에 명시하는 자원은 어떠한 경우에도 무조건 노드가 이 파드에 대해서 이 정도의 자원을 확보해야 한다는 의미이기에 이 값을 너무 크게 잡으면 유휴 자원으로 인해 리소스 낭비가 심해진다.\n만약 requests 를 무분별하게 크게 잡은 파드들이 많아서 노드의 자원의 크기를 넘어설 정도가 된다면 노드 자체가 가득 차서 파드 들이 스케쥴링되지 않는 상황이 발생할 수 있다는 점을 유의해야 한다.","2-limits--최대한도로-사용할-수-있는-제한limit을-의미#2) limits : 최대한도로 사용할 수 있는 제한(Limit)을 의미":"limits 는 파드가 노드 내에서 최대한도로 사용할 수 있는 제한(Limit)을 의미한다\n파드가 최대 어느 정도의 자원까지 사용이 가능한지를 명시한다.\nlimits 에 할당하는 자원들의 스펙은 꼭 쿠버네티스가 이 정도까지 확보해준다는 의미는 아니다.\n만약 노드에 자원이 여유가 없다면, 쿠버네티스는 requests 에 해당하는 한계까지만 채운다.\nlimits 에 설정하는 스펙은 자원이 무한대로 증가해서 노드를 잠식해버리는 상황을 방지할 수 있게끔 한계값을 설정해줄 수 있는 역할을 한다.\n만약 limits 를 설정하지 않으면 애플리케이션이 자원을 사용할 수 있는 자원의 상한이 정해지지 않았기 때문에 애플리케이션이 노드의 자원스펙을 모두 사용하게 되는 상황이 발생할 수 있다.\n노드의 자원을 모두 사용할 수 있다는 점이 장점으로 보일 수도 있지만 그 노드의 자원을 낭비하게 되기도 하고 다른 파드의 스케쥴링이 방해되기도 하고 노드의 자원이 얼마나 남아있는지 여부에 걸려있는지에 파드의 스케일링이 결정되는 계획되지 않은 스케쥴링이 될 수 있는 요소가 강하기에 가급적이면 limits 를 설정하는 것이 권장되는 편\n메모리 같은 요소의 경우 특정 프로세스에 자원이 할당이 되면은 사용하고 있는 자원이 별로 없더라도 잘 반납이 안되는 경우가 많기도 하기 때문에 그래서 문제가 심각해질 수도 있기에 가급적 limits 를 설정해주는 것이 권장되는 편","memory-cpu-의-단위-mi-gi-m#memory, cpu 의 단위 (Mi, Gi, m)":"memory 에는 Mi, Gi 를 주로 단위를 사용한다. 일반적으로는 Mi는 비트 단위 MB는 바이트 단위, Gi 는 비트 단위 GB 바이트 단위로 인식되는데, Mi 는 MB보다 조금 더 크게 잡힐 수 있고, Gi 는 GB 는 더 크게 잡힐 수 있다. JVM 메모리 지정시 대부분 MB, GB 단위로 지정하는데, Mi, Gi와 엄청나게 큰 숫자 차이는 나지 않는다.즉, 세부적으로 따지면 조금은 다른 의미이긴 하지만 MB, GB와 비슷한 단위로 파악하는 것이 완전하게 틀려지는 방식은 아니다.cpu 에는 주로 m 이라는 단위를 사용한다. 컨테이너가 할당받는 CPU Time 을 코어로 환산한다. 1 코어당 1000m 으로 환산해서 계산한다. \r\n예를 들어 1코어(=1000m) 기반의 노드에서 500m 은 1/2 코어를 의미한다. 만약 8코어(=8000m) 기반의 노드에서 2000m 은 1/5 x 8 = 5/8 = 1.6 코어 를 의미한다.","pod-의-성능-자원-replicas-설정-기준#Pod 의 성능, 자원, Replicas 설정 기준":"CPU, Memory 의 크기를 성능테스트 or 모니터링을 통해 결정\nPod 의 수량을 늘린다고 해서 성능을 선형적으로 올려주지는 않는다.\n비즈니스의 동작 방식에 따라 수직적, 수평적 스케일링의 효과가 달라진다.\n노드의 자원이 허용되는 한도 내에서 어느 정도의 여유를 두고 스케일링 설정을 하자.","replica-조정-deployment#Replica 조정 (Deployment)":"Deployment 에는 replicas 를 조정해서 Pod 의 수량을 조절할 수 있다.\nkind: Deployment\r\nmetadata:\r\n  name: my-app\r\nspec: ### spec 내의 replicas 를 수정해서 Pod 의 갯수를 조정할 수 있다.\r\n  replicas: 5\n주로 spec.replicas 를 조정해서 수량을 조정할 수 있다.아래의 명령어를 통해 동적으로 변경하는 것 역시 가능하다. 트래픽이 급작스럽게 증가했을 경우에 아래와 같은 명령어로 스케일링을 늘리기도 한다.\n$ kubectl scale deployment my-app --replicas=5\n이렇게 명령어로 진행한 내용은 실제 스펙에는 반영되지 않으므로 다음 배포 때에 늘려주었던 만큼의 레플리카가 실제로 필요하다면 스펙에 반영해서 배포를 진행하면 된다.replicas 숫자를 늘릴 경우에는 기존 파드에 영향 없이 새로운 파드가 추가되는 것으로 스케일링이 이뤄진다.","스케일링-꿀팁#스케일링 꿀팁":"초반에는 애플리케이션이 몇개의 파드로 실행되어야 하고 각각의 파드가 노드 내에서 어떤 사양으로 실행될 지를 정의하는 파드의 성능에 대한 수직적 스케일링과 파드의 갯수 등을 고려한 수평적 스케일링을 정의한다.운영 시에는 트래픽이 예상했던 것 보다 더 많이 들어올 수 있기 때문에 이 때부터는 수평적으로 스케일링을 하면서 대응을 해나간다.","cpu-resource-설정#CPU Resource 설정":"CPU 리소스의 경우 1core 또는 1000m(1000 밀리코어) 와 같은 단위로 표시된다.\r\nCPU 리소스 지정시에는 주로 m 이라는 단위를 사용해서 표기한다.컨테이너가 할당받는 CPU Time 을 코어로 환산한다. 1 코어당 1000m 으로 환산해서 계산한다. 노드의 CPU core 를 2로 지정한다고 해서 core 2개를 프로세스에 물리적으로 할당했다는 의미가 아니라는 점에 유의해야 한다. 노드가 가지고 있는 전체 코어 중에서 컨테이너가 요청한 CPU 코어 양에 맞게 CPU Time 을 비율로 따져서 비율에 맞도록 CPU Time 을 할당해준다.예를 들어 1코어(=1000m) 기반의 노드에서 500m 은 1/2 코어만큼의 CPU Time 을 의미한다. 만약 8코어(=8000m) 기반의 노드에서 2000m 은 2000/8000 = 1/4 = 0.25 코어만큼의 CPU Time 을 의미한다. 만약 1000m 을 지정하면 1 core 정도의 CPU Time을 할당받은 것을 의미한다.노드의 OS 입장에서는 컨테이너가 여러 개 떠있고 이 컨테이너들을 시분할을 통해서 동일하게 시간을 분배해주면서 컨텍스트 스위칭을 하면서 멀티테스킹을 해야 하는 방식이라는 것을 떠올리면 CPU Time 단위로 Core 를 분배하는 것에 대한 개념을 이해가 가능하다. 1000m 을 CPU 스펙에 정의하면 딱 잘라서 1Core 만 사용하는 싱글스레드 기반이라는 의미가 아니라 1 core 만큼의 CPU Time 을 사용하는 것을 의미한다.CPU Time 의 경우 정밀하게 CPU의 코어 만큼의 시간이 적용된다기보다는 조금 더 사용될 수도 있고 덜 사용될 수 도 있다.","jvm-메모리-설정#JVM 메모리 설정":"","주의할-점--레거시-jvm-은-request-limits-를-인식하지-못한다#주의할 점 : 레거시 JVM 은 request, limits 를 인식하지 못한다.":"주의할 점이 있다. Docker Container 환경이 나오기 전의 버전이나, Docker Container 환경이 고려되지 않은 비교적 레거시 버전의 JVM이 있는데 이 레거시 버전의 JVM 들은 파드에 지정한 requests, limits 를 인식하지 못한다. 이런 이유로 레거시 버전의 JVM 들은 노드의 물리적 메모리를 자신이 사용할 수 있는 메모리의 상한 값으로 생각하고 메모리를 사용하는 문제가 있다.즉, JVM 자신이 Docker container 내에서 구동 중이라는 것을 인식하는 기능이 추가되지 않은 레거시 버전의 JDK 버전들은 requests, limits 대신 JVM 옵션 (Xmx, Xms 등)을 따로 지정해줘야 한다.","가장-오래된-jvm-중-container-관련-기능이-패치되었을-가능성-있는-버전#가장 오래된 JVM 중 Container 관련 기능이 패치되었을 가능성 있는 버전":"JVM 8 버전 중에서는 가급적 후반버전에 이 부분에 대한 패치가 되어있을 수 있다.","usecontainersupport-옵션#UseContainerSupport 옵션":"JVM 이 Container 내에서 구동한다는 것을 명시하기 위해서 UseContainerSupport 옵션을 활성화하기도 하는데, 최신버전에 가까운 JVM 일수록 UseContainerSupport 옵션은 기본으로 활성화되어 있기에 최신버전에 가까운 JVM을 사용할 경우 굳이 UseContainerSupport 옵션을 명시하지 않아도 된다..UseContainerSupport 옵션을 비활성화 하는 것도 가능한데, 굳이 비활성화 하는 것으로 얻을 수 있는 이득이 없기에 비활성화 하는 것은 권장되지 않는다.","container-의-메모리-jvm-의-힙-메모리#Container 의 메모리, JVM 의 힙 메모리":"Container 의 메모리와 JVM 의 힙 메모리는 서로 다른 제한을 갖게 된다.\r\nJVM 은 힙 메모리 외에도 여러 종류의 메모리를 사용하는데, 만약 heap 메모리 옵션을 지정하지 않으면, Container 가 확보해둔 메모리의 4분의 1 정도를 heap 메모리로 할당한다. 이렇게 되면 예를 들어 Container 가 1GB 의 메모리를 확보했을 때 Heap 메모리는 256 MB 만 사용할 수 있게 된다.따라서 이렇게 256MB 만 사용할 수 있는 경우처럼 heap 메모리가 과도하게 작게 설정되는 경우를 방지하려면 별도의 JVM 옵션으로 Heap 메모리 옵션을 지정해줘야 한다.","힙-메모리-옵션들#힙 메모리 옵션들":"-Xms, -Xmx\n-XX:InitialRamPercentage, -XX:MaxRamPercentage","-xms--xmx#-Xms, -Xmx":"-Xms, -Xmx 의 경우 다소 오래된 버전의 JVM에서 사용되는 옵션이고 Heap 메모리를 고정적으로 할당하는 방식이다. 하지만, 보통 k8s 애플리케이션은 배포 시 마다 노드의 환경이나 배포 구성방식에 따라서 파드의 메모리가 바뀔 수도 있다. 이런 경우 고정적으로 할당된 -Xms, -Xmx 로 배포된 파드에 대한 이미지까지 재 빌드 후 배포해야 하게 된다.이렇게 -Xms, -Xmx 옵션이 인프라(노드)에 종속적인 면 때문에 -XX:InitialRamPercentage, -XX:MaxRamPercentage 이 추가됐다. 메모리 할당 사이즈를 비율로 지정할 수 있는 옵션이다.","-xxinitialrampercentage--xxmaxrampercentage#-XX:InitialRamPercentage, -XX:MaxRamPercentage":"-XX:InitialRamPercentage, -XX:MaxRamPercentage 을 동일하게 설정하는 경우가 있는데 이런 경우 힙 메모리를 고정적으로 할당해서 가비지 컬렉션 관련 이슈를 줄여줄 수 있다는 점에서 좋은 설정이 될 수 있다. (애플리케이션이 초기 구동 후 계속해서 힙 메모리가 필요할 때마다 키워나가기 때문)-XX:InitialRamPercentage를 50% 로 줄지, -XX:MaxRamPercentage 를 70% 로 줄지 이런 비율들은 컨테이너에 어느 정도의 메모리 자원이 비율이 할당이 되었는지 수치와 애플리케이션이 사용하는 힙메모리의 비율, 논 힙 메모리를 어느 정도 사용하는지 등의 크기에 따라서 달라질 수 있다.일반적으로 50% 정도의 수치에서부터 테스트를 해보면서 모니터링 도구를 통해 수치를 확인해가면서 힙 메모리를 더줘야 할지 줄여야할 지 이런 비율등에 대해 적정 비율을 찾아가면서 높여가면서 설정하는 것이 좋은 설정이다.이 비율을 처음부터 너무 높게 적용할 경우 Non Heap 메모리에 할당될 메모리의 비율이 줄어들게 되는데 이런 경우 애플리케이션 자체가 정상적으로 기동되지 않을 수 있기 때문에 이런 경우에 대해서는 조심을 해야 한다.","-xxexitonoutofmemoryerror#-XX:+ExitOnOutOfMemoryError":"애플리케이션이 받는 트래픽이 점점 증가하면서 메모리 사용량이 늘어나다가 결국은 할당된 메모리를 모두 사용하게 되었을 경우를 생각해보자.자바의 경우 메모리 사용량이 상한 값에 도달하게 될 경우 가비지컬렉션이 발생해서 CPU 사용량이 늘어나게 되고 가비지컬렉션을 통해서 메모리를 확보하기는 하지만 확보하는 메모리보다 메모리 사용량이 계속해서 늘어나게 되면 결국은 OutOfMemoryError 가 발생하게 된다.이 때 조심해야 할 점은 JVM 에서 OutOfMemoryError 가 발생하더라도 쿠버네티스가 여기에 대해서 별도의 조치를 취하지는 않는다. 쿠버네티스의 경우 컨테이너의 전체 메모리 사용으로 인해 Out Of Memory 이슈가 생기면 쿠버네티스 레벨에서 컨테이너를 재시작 시켜주지만, 컨테이너의 메모리에서 -XX:MaxRamPercentage 옵션을 통해 할당한 힙 메모리 사용량이 가득 찼을 때는 힙메모리가 가득찬 것이지 컨테이너의 전체 메모리가 가득찬 것이 아니기 때문에 JVM 애서 OOM 이 발생하더라도 컨테이너에서는 OOM 이 발생하지는 않기에 컨테이너가 재시작되지는 않는 이슈가 있다.JVM 에서 OOM이 발생하는 경우에는 JVM 이 공유되거나 프로세스가 종료된다거나 그런 것은 아닌데 OOM이 발생하는 JVM 을 그대로 방치하는 것은 오류가 발생할 소지가 있다.이런 경우 JVM 에 -XX:+ExitOnOutOfMemoryError 을 지정하면 OOM이 발생했을 때 해당 컨테이너가 재시작 될수 있게끔 명시할 수 있다는 장점이 있다."}},"/kubernetes-overview/kubernetes-scaling":{"title":"Kubernetes Scaling","data":{"":"Doc 문서 정리 직전에 어떻게 정리할지 그냥 요약해봄","스케일-조정#스케일 조정":"일반적으로 이야기하는 스케일링은 아래의 두가지 방식들이 있다.\nVertical Scaling : 수직적 스케일링\nHorizontal Scaling : 수평적 스케일링","수직적-스케일링-vertical-scaling#수직적 스케일링 (Vertical Scaling)":"인스턴스 하나의 자원의 할당량을 조절하는 것\r\n작업을 처리하는 인스턴스의 성능을 올리는 것을 스케일 업 이라는 표현으로도 부른다.\r\n레거시 시스템에서 흔히 보이는 구조\n대용량 배치, 일괄 대사 작업 등을 수행시 분산 트랜잭션 도입이 힘들 경우에도 이런 구조를 선택하는 경우가 있음\n수평적 스케일링을 도입하기에 앞서 시간적,인적 요소 고려시 임시적으로 도입하는 경우도 있음","수평적-스케일링-horizontal-scaling#수평적 스케일링 (Horizontal Scaling)":"작업을 처리하는 인스턴스의 수를 늘리거나 줄이는 것\r\n작업을 처리하는 인스턴스의 수를 늘리는 것을 스케일 아웃 이라는 표현으로도 부른다.\r\n클라우드 기반의 인프라가 대중화되면서 대중적으로 선호되는 스케일링 방식\n스케일아웃 기반의 애플리케이션을 구성하기 위해 일반적으로 Stateless 한 애플리케이션 구성, 서비스가 네트워크를 분배하는 방식 설계, 우아한 종료, 컨테이너의 빠른 시작 과 같은 요소들을 고려하는 편임\n수평적 스케일링을 하더라도 파드(Pod) 하나에 대한 스케일 업 역시 스펙을 어느 정도는 고려가 잘 되어야 하고, 초기 구성시 어느 정도의 부하를 견디는지 측정을 해보는 것도 중요함","kubernetes-의-오토스케일링#Kubernetes 의 오토스케일링":"kubernetes 의 오토스케일링은 Pod 단위로 수행된다. 쿠버네티스의 오토스케일러는 파드의 자원 상황을 모니터링하고 모니터링 한 결과에 따라 파드의 사양, 수량을 결정하게 된다. 쿠버네티스의 오토스케일링은 파드 단위로 사양, 수량을 조절하는 작업들이 유연하게 돌아가며, 이런 작업들이 자동으로 수행되도 문제가 발생하지 않도록 시스템 내부적으로 파드의 우아한 종료, 프로브 설정, 서비스에 네트워크가 분배되는 것과 관련된 것들이 유기적으로 잘 돌아가도록 내부적으로 잘 구성되어 있다. 쿠버네티스 환경에서의 오토스케일링은 적절한 파드의 사양, 수량을 결정해서 배포해두면 그 이후의 작업은 유연하게 넘어갈 수 있고 쿠버네티스 내에서의 파드의 사양, 수량을 결정하는 정책은 굉장히 유연한 편에 속한다.일반적인 오토스케일링 시스템에서는 스케일링시에 발생하는 네트워크 순단, 애플리케이션 다운, 스케일링 후에 잘 동작하는지 모니터링을 한다던가 하는 이런 것들이 어려웠었는데 쿠버네티스의 오토스케일링은 이런 것들에 대한 문제들을 고민하지 않아도 된다는 점이 장점이다.쿠버네티스는 두 종류의 오토스케일링을 지원한다.\nHPA (Horizontal Pod Autoscaler)\n오래전 부터 기본기능으로 지원됐던 기능이다. 현재는 두번째 버전까지 나와있다.\nVPA (Vertical Pod Autoscaler)\n베타버전만 존재하고 특정 클러스터 플랫폼에는 포함되지 않은 기능이다.\nPod 의 자원 사용량에 따라 파드의 CPU, Memory 사용량을 조절\n자원사용량을 변경하기에 컨테이너가 재시작하게 되고 HPA와 함께 사용할 경우 충돌가능성이 높다.","오토스케일링-절차#오토스케일링 절차":"Monitor\n파드의 상태를 모니터링하고 지표를 수집하는 단계\nAnalyze\n모니터링 단계에서 수집된 지표를 분석\nPlan\n파드의 적절한 사양을 계산\nExecute\n조정 작업 수행\n이 과정은 쿠버네티스의 오토스케일러는 계속해서 수행을 한다.","오토스케일러-리소스-정의#오토스케일러 리소스 정의":"오토스케일러 리소스는 아래와 같이 정의한다.\napiVersion: autoscaling/v2 ## 1)\r\nkind: HorizontalPodAutoscaler\r\nmetadata:\r\n  name: my-hpa\r\nspec:\r\n  scaleTargetRef: ## 2)\r\n    apiVersion: apps/v1\r\n    kind: Deployment\r\n    name: my-app\r\n  minReplicas: 1 ## 3)\r\n  maxReplicas: 5 ## 3) \r\n  metrics: ## 4) \r\n    - type: Resource\r\n      resource:\r\n        name: cpu\r\n        target:\r\n          type: Utilization\r\n          averageUtilization: 50 ## 5)\n1) apiVersion: autoscaling/v2 : Autoscaler 버전\nAutoscaler 의 버전을 명시한다.\n꽤 오랫동안 v1 을 사용했고, v2가 나온지는 얼마되지 않았다. 인터넷 자료들 대부분이 v1 을 사용하는 경우가 많다.\nv1 오토스케일러의 경우 대부분 CPU 점유율 기준으로 오토스케일링을 하고, 설정이 간단했었다.\nv2 오토스케일러의 경우 다양한 자원들에 대해 오토스케일링이 가능하고 설정도 다양해졌다.\nautoscaler 버전은 v1을 사용할지 v2를 사용할지  가능 여부는 쿠버네티스 클러스터 버전에 따라서 달라질 수 있다.\nv2 가 가지고 있는 이점이 많기 때문에 클러스터가 v2를 지원한다면 가급적 v2 를 사용하는 것이 권장되는 편이다.\n2) spec.scaleTargetRef\nspec.scaleTargetRef 는 스케일링이 가능한 리소스 타입이어야 한다.\n즉, replicas 필드를 가지고 있는 리소스 타입에만 적용이 가능하기에 Statefulset, Deployment, ReplicaSet 에만 spec.scaleTargetRef 를 적용가능하다.\n대부분의 경우 Deployment 를 지정하는 편이다.\n만약 Statefulset 을 지정할 경우 Statefulset 에 연결할 PV를 미리 할당을 해두거나 동적할당을 해야 한다는 점을 고려해서 신중하게 설정해야 한다.\n3) minReplicas, maxReplicas : HPA 의 조정 범위를 지정\n레플리카(복제)의 범위를 지정\n파드를 최소 몇개, 최대 몇개까지로 할지 지정\nminReplicas : 오토스케일러를 사용하지 않을 때 평소에 사용하는 오토스케일러의 수량을 지정\nmaxReplicas : 노드에 무리를 주지 않는 선에서 충분한 수치를 지정해주는 것이 좋다.\n만약 새벽 시간대 처럼 사용량이 적은 시점에 파드 수량을 줄이고 확보한 자원들을 통해 배치작업 등을 수행하려 한다면 minReplicas 를 평소에 줄여두는 것도 하나의 방법이 될수 있는데 주의할 점이 있다. minReplicas를 극도로 줄여두었다가 트래픽 급증 시점에 파드의 기동이 계속해서 발생하는 것으로 인한 처리 지연현상이 발생할 수 있는 이슈가 있다는 점이다. minReplicas 가 너무 작은 것으로 인해 파드의 생성이 자주 이뤄줘야 하는데, 트래픽이 완만하게 올라가는 시점(e.g. 새벽)이 아닌 트래픽이 급증하는 시점(e.g. 출근시간)에는 단점으로 작용할 수 있다. 파드가 구동되는 데에 일정 시간이 걸리기에 파드가 계속해서 구동되기를 기다리는 것으로 인한 지연이 발생할 수 있기 때문이다.\n보통 autoscaler 의 스케일 범위는 자원을 절약하기 위한 수준의 범위를 지정하기 보다는 평소에 autoscaler 를 안쓸때 ReplicaSet에 일반적으로 지정하는 수량과 동일한 개념으로 적용하면 되고 갑자기 트래픽이 급증할 때 대응할 수 있는 수량을 최대 수량으로 잡는 것이 좋다.\n4) metrics\nAutoscaling 을 할 때 기준이 될 수 있는 자원과 그 기준값을 지정할 때 사용된다.\nv1 에서는 target.averageUtilization 라는 CPU 퍼센트를 간편하게 지정하는 용도로 사용되는 경우가 많았지만 CPU 의 다른 요소들을 설정하는 것이 복잡한 편이었다.\nv2 에서는 Resource 라는 타입을 별개의 타입으로 지정해서 사용할 수 있게 되었고 v2 에서 일반적으로 사용하는 메트릭은 cpu, Memory 가 보편적이고 이 외에 메트릭이 필요할 때는 커스텀 설정을 통해 추가가 가능하다. 예를 들면 ingress 에서 들어오는 트래픽을 기준으로 파드의 스케일링 설정이 가능하다.\ntarget 에 지정한 Utilization 이라는 타입은 절대적인 수치나 전체적으로 자원을 사용하는 비율이라든지 이런 것들을 지정할 수 있다. 단위는 % 단위로 입력한다.\naverageUtilization 은 예를 들면 이런 개념이다. \"파드 들의 평균적인 CPU 사용량이 50% 를 넘지 않으면서 최소 50% 를 소비했으면 한다\" 는 요구조건이 있다면 averageUtilization 을 50 (%) 로 지정하는 편이다.\n참고) Utilization 관련한 자료들은 아래의 자료들이 있다.\n쿠버네티스 CPU, Memory 관리하기(feat. 사용량 확인, limit, 리소스 모니터링)\n쿠버네티스 (Kubernetes) 배포를 위한 고급설정\n5) averageUtilization\n평균적인 값이 이 정도가 되어야 함을 정의하며 단위는 % 이다.\n위의 예제에서는 spec.metrics[0] 가 cpu 타입인데 여기에 대해 averageUtilization 을 50으로 정의했다는 것은 평균적인 CPU 점유율이 50%가 되어야 함을 의미한다.","averageutilization-기반-스케일링#averageUtilization 기반 스케일링":"오토스케일러가 파드(Pod)의 수량을 결정하는 공식은 아래와 같다. (생각보다 단순한 방식의 계산이다)\ndesiredReplicas = ceil(currentReplica * (CurrentPodUtilization/Target))\n(CurrentPodUtilization/Target)\r\nCurrentPodUtilization 은 전체 파드가 사용하고 있는 자원 사용량의 평균이다.\r\n그리고 CurrentPodUtilization 을 Target(목표 자원사용량) 으로 나눈다.\ncurrentReplica * (CurrentPodUtilization/Target)\r\n위에서 구한 (CurrentPodUtilization/Target) 에 현재 레플리카로 기동된 Pod 의 개수를 곱해준다.\nceil(currentReplica * (CurrentPodUtilization/Target))\r\ncurrentReplica * (CurrentPodUtilization/Target) 을 올림처리한다. 이렇게 구해진 수치가 파드의 Autoscaling 에 적용되는 averateUtilization 값이 된다.\n이 과정을 예를 들어서 그림으로 표현해보면 아래와 같다.","목표-자원사용량을-넘어서도-그대로-유지되는-경우#목표 자원사용량을 넘어서도 그대로 유지되는 경우":"maxReplicas = 3 이고 averageUtilization = 50 일때 Pod 2기가 각각 90%, 70% 의 CPU를 점유한 상황을 살펴보면 아래와 같다.","파드의-수량을-축소하는-경우#파드의 수량을 축소하는 경우":"maxReplicas = 3 이고 averageUtilization = 50 일때 Pod 2기가 각각 20%, 15%, 10% 의 CPU를 점유한 상황을 살펴보면 아래와 같다.","horizontalpodautoscaler-사용-시-주의점#HorizontalPodAutoScaler 사용 시 주의점":"","오토스케일링은-즉시-발생하지-않는다는-점에-주의#오토스케일링은 즉시 발생하지 않는다는 점에 주의":"오토스케일링은 즉각적으로 발생하지 않는다. 파드의 상태에 따라서 파드를 늘렸다가 줄였다가를 즉각적으로 수행하는 것을 반복하는 것으로 인해 자원 사용량이 계속해서 변화한다면 노드 상황도 불안정해질 수 있고 노드 내의 자원도 불안정해지고 시스템도 불안정해질 위험이 존재한다.이런 이유로 인해 오토스케일러는 파드의 자원 성능을 기준으로 +/- 5% 수준의 오차범위 내에서는 조정을 하지 않도록 임계치(Margin)을 두어 잦은 스케일링이 일어나지 않게끔 하고 있다. 한번 오토스케일링이 일어난 후에는 다음 오토스케일링을 수행하기 전 까지 휴식기간을 주어서 잦은 오토스케일링을 방지하기도 하는데 이 휴식기간 수치에 대한 옵션은 수정이 가능하다.트래픽이 지속적으로 급속도로 증가하는 상황에서 오토스케일링을 거친 후에 거치는 휴식기간이 길면 오토스케일링이 수행되지 않는 것으로인한 처리 지연이 길어질 수 있는 상황이 생길 수도 있다.이런 이유로 오토스케일링은 1차적인 기본 설정으로 두어야하고 세부적인 설정을 꼭 해주어야 한다.","오토스케일링을-고려해서-애플리케이션을-개발해야-함#오토스케일링을 고려해서 애플리케이션을 개발해야 함":"오토 스케일링은 임의의 시점에 파드가 새로 생성될 수도 있고 임의의 시점에 파드가 종료될 수도 있다. 따라서 파드를 새로 추가해서 스케일 아웃을 통해 트래픽을 나눠서 처리할 수 있는 구조여야 하고 Graceful Shutdown (우아한 종료) 로직을 통해 언제 종료되더라도 종료 시에 해야 하는 필요한 로직처리를 꼭 해줘야 한다.","커스텀-메트릭을-활용해서-기능을-확장하는-방법#커스텀 메트릭을 활용해서 기능을 확장하는 방법":"v1 버전의 오토스케일러에서는 cpu 기반으로 오토스케일링 기능을 제공했다. 따라서 대부분 오토스케일링을 CPU 기준으로 오토스케일링 리소스를 정의하는 경우가 많다. 일반적으로 스레드 기반의 애플리케이션에서는 스레드 점유율이 높아지면 CPU 점유율이 높아지기도 하기에 CPU 기준으로 오토스케일링을 하는 것이 무리가 있다던가 하는 것은 아니지만 애플리케이션의 작업 성격에 따라서 CPU 바운드 보다는 커넥션의 획득이나 IO 작업으로 인한 대기 문제로 인해 메모리 사용량을 통한 오토스케일링을 지정하는 것이 오히려 더 유의미한 경우가 있다. 따라서 CPU 외에도 메모리를 커스텀하게 적절하게 설정하는 것이 필요하다.CPU, 메모리 외에도 다른 지표를 사용해야 효율적으로 스케일링을 할 수 있겠다는 생각이 든다면 커스텀 메트릭을 사용하는 것이 추천된다. 커스텀 메트릭 사용시 지표 수집에 대한 세팅을 위해 별도의 커스텀 매트릭 플러그인 설치등을 해주는 것으로 커스텀 메트릭을 사용하는 것이 가능하다."}},"/kubernetes-overview/storage-class":{"title":"Storage Class","data":{"storageclass#StorageClass":"","local-path#local-path":"kind 에는 local-path 타입의 StorageClass 가 설치되어 있습니다. kind 클러스터의 버전에 따라 다르겠지만, 이번 예제에서 사용한 kind 클러스터에서 나타난 local-path 타입의 StorageClass 의 이름은 standard 라는 이름으로 설치되어 있습니다.\r\nStorageClass 를 조회하는 명령은 kubectl get sc 입니다.\n$ kubectl get sc\r\nNAME                 PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\r\nstandard (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  11m\n위의 결과를 보면 Kind 클러스터 내에 local-path 타입으로 디폴트로 설치된 StorageClass 의 이름은 \"standard\" 입니다.\r\n아래에서 정의할 PVC 정의 yaml 에서 spec.storageClassName 은 반드시 위에서 조회해서 알게된 standard 로 지정해주면 됩니다.\n어떻게 정의되어 있는지 확인해보면 아래와 같습니다.\n$ kubectl get sc standard -oyaml\r\napiVersion: storage.k8s.io/v1\r\nkind: StorageClass\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io/last-applied-configuration: |\r\n      {\"apiVersion\":\"storage.k8s.io/v1\",\"kind\":\"StorageClass\",\"metadata\":{\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"},\"name\":\"standard\"},\"provisioner\":\"rancher.io/local-path\",\"reclaimPolicy\":\"Delete\",\"volumeBindingMode\":\"WaitForFirstConsumer\"}\r\n    storageclass.kubernetes.io/is-default-class: \"true\"\r\n  creationTimestamp: \"2024-02-01T07:44:21Z\"\r\n  name: standard\r\n  resourceVersion: \"286\"\r\n  uid: 5b06a540-194d-4f69-b93b-6435b761812b\r\nprovisioner: rancher.io/local-path\r\nreclaimPolicy: Delete\r\nvolumeBindingMode: WaitForFirstConsumer\n이번에는 local-path 타입의 pvc 를 한번 만들어보겠습니다.\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: k8shelloboot-pvc-local-path\r\nspec:\r\n  storageClassName: standard # kubectl get sc 명령을 통해 나타나는 storeclass 중 하나를 선택했다.\r\n  # storageClassName: local-path\r\n  # storageClassName: \"\"\r\n  accessModes:\r\n    - ReadWriteOnce\r\n  resources:\r\n    requests:\r\n      storage: 500Mi\r\nspec.storageClassName\nkubectl get sc 명령을 통해 나타나는 StorageClass 목록에서 원하는 StorageClass 를 선택합니다.\n이번 문서의 제일 처음에 kind 클러스터에서 StorageClass 중 설치된 것이 있는지 조회해봤고 standard 가 default 였기에 storageClassName 을 standard 로 지정해줬습니다.\nkubectl apply 를 통해 적용해봅니다.\n$ kubectl apply -f k8shelloboot-pvc-local-path.yml\r\npersistentvolumeclaim/k8shelloboot-pvc-local-path created\n만들어진 pvc 를 확인해보겠습니다.\n$ kubectl get pvc\r\nNAME                          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE\r\nk8shelloboot-pvc-local-path   Pending                                      local-path     10s\n현재 Pending 상태입니다. Pod가 PVC를 사용할 때 동적으로 볼륨이 생성됩니다.\ndeployment 를 기동해보기 위해 아래의 deployment 정의서를 작성합니다.\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: k8shelloboot-app-deploy\r\nspec:\r\n  selector:\r\n    matchLabels:\r\n      app: k8shelloboot-app\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: k8shelloboot-app\r\n    spec:\r\n      volumes:\r\n        # 1)\r\n        - name: log-volume\r\n          persistentVolumeClaim:\r\n            claimName: k8shelloboot-pvc-local-path\r\n      containers:\r\n      - name: k8shelloboot\r\n        image: chagchagchag/k8shelloboot:0.0.1\r\n        imagePullPolicy: Always\r\n        volumeMounts:\r\n          - mountPath: /app/volume\r\n            name: log-volume\r\n        ports:\r\n        - containerPort: 8080\n1)\nspec.volumes 내에 volume 을 하나 추가해줬습니다.\nvolume 의 이름(name)은 log-volume 이라고 정의해줬고, persistentVolumeClame.clameName 을 지정해서 사용하려는 PersistentVolumeClaim 의 이름을 명시했습니다.\n작성한 리소스 정의서를 kubectl 을 통해 kubernetes 클러스터에 적용합니다.\n$ kubectl apply -f k8shelloboot-deploy.yml\r\ndeployment.apps/k8shelloboot-app-deploy created\ndeployment 가 제대로 구동되었는지 확인해보겠습니다.\n$ kubectl get all\r\nNAME                                           READY   STATUS    RESTARTS   AGE\r\npod/k8shelloboot-app-deploy-7f67896bf7-wqxs9   1/1     Running   0          7s\r\n\r\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\r\nservice/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   54m\r\n\r\nNAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\r\ndeployment.apps/k8shelloboot-app-deploy   1/1     1            1           7s\r\n\r\nNAME                                                 DESIRED   CURRENT   READY   AGE\r\nreplicaset.apps/k8shelloboot-app-deploy-7f67896bf7   1         1         1       7s\n일단 pod 의 상태를 보면 정상적으로 Running 상태에 진입했다는 사실을 확인했습니다.\n이번에는 pod 내부로 접속해서 API가 잘 동작하는지 로그가 잘 찍혔는지 확인해봅니다. Service와 Ingress 를 모두 만들어서 호스트 PC에서 확인해보면 멋있어보이겠지만, 개념설명의 호흡이 길어져서 요지를 알 수 없을 수 있기에 pod에 접속해서 직접 로그를 확인하는 방식으로 동작을 확인해봅니다.\n## pod 접속\r\n$ kubectl exec -it k8shelloboot-app-deploy-7f67896bf7-wqxs9 -- bash\r\nbash-4.2#\r\n...\r\n\r\n## API 호출 시 정상응답하는지 확인\r\nbash-4.2# curl http://localhost:8080/healthcheck\r\nOK\r\n...\r\n\r\n## 로그파일 내용 확인\r\nbash-4.2# cat /app/volume/cache-log.log\r\n2024-02-01T08:47:51.183360936>>> write OK\r\n...\r\n\r\n## API 호출 시 정상응답하는지 확인\r\nbash-4.2# curl http://localhost:8080/healthcheck\r\nOK\r\n\r\n## 로그파일 내용 확인 (로그파일에 모두 잘 기록되었다.)\r\nbash-4.2# cat /app/volume/cache-log.log\r\n2024-02-01T08:47:51.183360936>>> write OK\r\n2024-02-01T08:48:19.245463098>>> write OK"}},"/kubernetes-overview/window-directory-k8s-mount":{"title":"kubernetes hostPath 지정시 윈도우 WSL 디렉터리 경로","data":{"참고자료#참고자료":"Kubernetes on docker for windows, persistent volume with hostPath gives Operation not permitted\nWSL2 Kubernetes 환경에서 Windows 경로 Mount 하기\nKubernetes how to correctly mount windows path in wsl2 backed environment\nWindows 10 20H2 Docker Desktop WSL2 enable 환경에서 Kubernetes volume hostPath 연결 오류 해결\nKubernetes on docker for windows, persistent volume with hostPath gives Operation not permitted\nwsl2 kubernetes volume hostpath 못잡는 문제 해결방법\nhttps://www.inflearn.com/questions/98495/access-i-s-denied-문제\nError response from daemon: mkdir C:\\Program Files\\Git\\opt: Access is denied\npath for PersistentVolume in Kubernetes .yaml in Windows","윈도우os-에서-hostpath-경로#윈도우OS 에서 hostPath 경로":"쿠버네티스 역시 결국은 Docker 엔진으로 동작하는데, 윈도우 기반의 Docker 엔진에서 WSL 의 어느 디렉터리를 참조하는지가 꽤 까다롭습니다.\r\n디렉터리를 참조하는 경로는 아래와 같이 구성됩니다.\n/run/desktop/mnt/host/c/PATH/TO/FILE\n예를 들어 HOST PC 의 /v/000.env/volume 을 파드 내에서 마운트할 때 hostPath 의 path 는 아래와 같이 지정해줍니다.\n/run/desktop/mnt/host/v/000.env/volume"}},"/setup/local-k8s-setup":{"title":"Local K8s Setup","data":{"로컬-k8s-셋업#로컬 k8s 셋업":"","notice#Notice":"아래에서 진행하는 모든 내용들은 chagchagchag/fibonacci-backend 을 Clone 받은 후에 진행합니다.","설치-스크립트#설치 스크립트":"로컬 환경에 kind 클러스터 설치를 setup.sh 파일 하나를 실행하는 것으로 가능하게 하는 방법입니다. 아래와 같은 명령을 실행합니다.\ncd cluster\r\nsource setup.sh\n아래에서부터는 Cluster 정의가 어떻게 되는지, ArgoCD가 NodePort 로 어떻게 접속하는지, API 가 어떤 Service 에 붙어서 이 Service 를 어떤 ingress 에서 처리하는지를 정의합니다.","argocd-없이-백엔드-애플리케이션만-테스트해볼때#ArgoCD 없이 백엔드 애플리케이션만 테스트해볼때":"ArgoCD 까지 모두 띄워둔 후 작업을 하기에는 개발작업만 할 때에는 조금 부담스럽습니다.\r\nArgoCD 없이 백엔드 애플리케이션 개발만을 위한 클러스터 구성은 아래의 명령어로 가능합니다.\ncd cluster\r\nsource create-single-cluster.sh\ncluster/create-single-cluster.sh 파일의 내용은 아래와 같습니다.\necho \"\"\r\necho \"=== create Cluster & Ingress-Nginx (Ingress Controller) ===\"\r\necho \"[create] cluster creating...\"\r\nkind create cluster --name fibonacci-cluster --config=cluster.yml\r\n\r\necho \"\"\r\necho \"[create] create ingress-nginx\"\r\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\r\n\r\necho \"\"\r\necho \"[wait] wait ingress-nginx standby\"\r\nkubectl wait --namespace ingress-nginx \\\r\n  --for=condition=ready pod \\\r\n  --selector=app.kubernetes.io/component=controller \\\r\n  --timeout=90s\nsingle-cluster.yml 파일의 내용입니다.\nkind: Cluster\r\napiVersion: kind.x-k8s.io/v1alpha4\r\nnodes:\r\n  - role: control-plane\r\n    kubeadmConfigPatches:\r\n      - |\r\n        kind: InitConfiguration\r\n        nodeRegistration:\r\n          kubeletExtraArgs:\r\n            node-labels: \"ingress-ready=true\"\r\n    extraPortMappings:\r\n      - containerPort: 80 # nginx-ingress 가 인식하는 포트\r\n        hostPort: 80 # API 접속시 호스트 PC 의 80 포트로 접속\r\n        protocol: TCP","포트-사용현황#포트 사용현황":"30009 포트\nArgoCD Server\nlocalhost:30009\n80 포트\n백엔드 애플리케이션\nlocalhost/probe/healthcheck\n뒤에서 설명하는 예제들에서 사용하는 예제 애플리케이션입니다. 이번 문서에서는 설치하지 않습니다.","cluster-정의#Cluster 정의":"이 부분은 Kind 에 대한 내용을 파악하는 것인데, 귀찮다면 건너뛰셔도 됩니다.cluster.yml\nkind: Cluster\r\napiVersion: kind.x-k8s.io/v1alpha4\r\nnodes:\r\n  - role: control-plane\r\n    kubeadmConfigPatches:\r\n      - |\r\n        kind: InitConfiguration\r\n        nodeRegistration:\r\n          kubeletExtraArgs:\r\n            node-labels: \"ingress-ready=true\"\r\n    extraPortMappings:\r\n      - containerPort: 30009 # ArgoCD Node Port 를 위한 바인딩\r\n        hostPort: 30009 # ArgoCD 접속시 호스트 PC에서도 30009 로 접속\r\n        protocol: TCP\r\n      - containerPort: 80 # nginx-ingress 가 인식하는 포트\r\n        hostPort: 80 # API 접속시 호스트 PC 의 80 포트로 접속\r\n        protocol: TCP\r\n  - role: worker\r\n# - role: worker\r\n# - role: worker\nKind 클러스터는 실제로 구동될 때 하나의 Container 로 동작합니다. Kind 클러스터를 실행한 후 Docker Desktop 을 열어서 확인하면 실제 생성된 Kind 클러스터를 확인 가능합니다.extraPortMappings[i].containerPort\nKind 클러스터 컨테이너 입장에서 외부로 노출할 포트를 의미합니다. 즉 containerPort 라는 것은 Kind 클러스터 컨테이너의 Port 를 의미합니다.\nextraPortMappings[i].hostPort\n호스트 PC 즉, 개발 PC 내에서 Kind 클러스터로 접속 시에 사용할 Port 를 의미합니다.\nnodes[i].role\ncontrol-plane, worker 등을 지정해줄 수 있습니다. 만약 Cluster 가 하나의 Container 로만 구성되게끔 하려면 worker 를 사용하지 않아도 됩니다.\n경험상 단순한 백엔드 애플리케이션이나 Frontend 애플리케이션을 테스트할 때에는 worker 까지는 필요 없었고 control-plane 하나만으로도 충분히 테스트가 가능했습니다. 다만 ArgoCD 와 함께 구동 시에는 worker 가 적어도 1기 이상은 있어야 합니다.\nnext.js, nuxt.js 의 경우에도 worker 1기 이상은 있어야 파드가 정상적으로 기동되었던 것으로 기억합니다. 요즘 프론트엔드 프레임워크가 옛날 처럼 단순하고 정적인 레벨을 넘어섰기에 리소스도 어느 정도 잡아먹는 듯 합니다.","cluster-생성-cluster-내에-ingress-nginx-연동#cluster 생성, cluster 내에 ingress-nginx 연동":"클러스터는 아래와 같이 생성 가능합니다.\nkind create cluster --name fibonacci-cluster --config=cluster.yml\n이렇게 생성된 클러스터는 아래와 같이 조회 가능합니다.\nkind get clusters\nkind 클러스터가 외부와 통신이 가능하려면 Ingress 컨트롤러가 필요합니다. Ingress 컨트롤러 중 가장 대중적으로 알려진 ingress-nginx 를 kind 클러스터 내에 설치하는 명령어는 아래와 같습니다.\n## ingress-nginx 를 설치합니다.\r\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\r\n\r\n\r\n## ingress-nginx 내의 pod 이 로딩 될때 까지 kubectl wait 을 수행합니다.\r\n## 타임아웃은 90초로 주었습니다.\r\nkubectl wait --namespace ingress-nginx \\\r\n  --for=condition=ready pod \\\r\n  --selector=app.kubernetes.io/component=controller \\\r\n  --timeout=90s\n여기까지의 명령어들은 소스코드 리포지터리 내의 cluster/create-cluster.sh 파일 내에 정의해두었고, 그 내용은 아래와 같습니다.\r\ncluster/create-cluster.sh\necho \"\"\r\necho \"=== create Cluster & Ingress-Nginx (Ingress Controller) ===\"\r\necho \"[create] cluster creating...\"\r\nkind create cluster --name fibonacci-cluster --config=cluster.yml\r\n\r\necho \"\"\r\necho \"[create] create ingress-nginx\"\r\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\r\n\r\necho \"\"\r\necho \"[wait] wait ingress-nginx standby\"\r\nkubectl wait --namespace ingress-nginx \\\r\n  --for=condition=ready pod \\\r\n  --selector=app.kubernetes.io/component=controller \\\r\n  --timeout=90s","클러스터-내에-argocd-설치#클러스터 내에 ArgoCD 설치":"","argocd-설치#argocd 설치":"argocd 를 설치하기 위해서는 먼저 argocd 라는 이름의 namespace 를 하나 생성해줘야 합니다.\nkubectl create namespace argocd\n이번에는 argocd 를 구성하는 리소스들을 클러스터 내에 생성하도록 kubectl 로 요청하는 절차입니다.\nkubectl -n argocd apply -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n이번에는 이렇게 생성된 argocd 관련 리소스 들 에 대해 SSL 옵션을 끄는 명령입니다.\nkubectl -n argocd patch deployment argocd-server --type json -p='[{\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/0/args\",\"value\":[\"/usr/local/bin/argocd-server\",\"--insecure\"]}]'\n생성된 argocd 에서는 admin 이라는 사용자에 대해 초기 password 가 부여되는데 초기 패스워드는 아래의 명령어로 확인 가능합니다.\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n여기까지 ArgoCD를 모두 설치했습니다. 이렇게 클러스터 내에 설치한 ArgoCD 는 Kind 클러스터 외부, 즉, 호스트 PC(개발PC)에서 접속 가능하려면 Ingress, Nodeport, ClusterIP 등을 이용해 접근이 가능합니다.Ingress 를 사용할 경우 80, 443 포트만 매핑이 가능한데, 80, 443 포트에 대한 / 경로 접근은 애플리케이션을 위해 예약되어 있다. 따라서 이번 예제에서는 ArgoCD 에 Ingress 를 통해 접근하게끔 하지 않을 예정입니다.ArgoCD 를 / 경로로 Ingress 를 통해 ArgoCD 하나를 위해 독자적으로 80, 443 포트로 접근하는 예제가 필요하다면 추후 별도로 Kind 클러스터 세팅에 대해서 정리할 예정이기에 별도의 문서를 참고해주시기 바랍니다.\nClusterIP 를 사용할 경우 클러스터 내에 배포된 argocd-server 의 IP를 알고 있다면 접근이 가능합니다. 이렇게 한다면, 매번 유동적으로 바뀌는 IP를 직접 CLI 로 체크해서 접속해야 하는 불편함이 있습니다.\n이번 예제에서 사용하는 ArgoCD 는  30009 포트에 매핑한 NodePort 를 사용합니다. NodePort를 통해서 ArgoCD가 클러스터 외부와 30009 포트의 / 을 통해 통신을 할 수 있도록 구성했습니다.","nodeport-정의#nodeport 정의":"포트 매핑\n호스트 PC → [클러스터 30009 : NodePort 30009 → 80:8080(argocd-server)]\n위에서 이야기 했듯 이번 예제 프로젝트에서 ArgoCD 는 NodePort 를 통해서 ArgoCD가 클러스터 외부와 30009 포트의 / 경로를 통해 통신을 할 수 있도록 구성했습니다.이렇게 정의한 NodePort 의 역할은 Kind 클러스터 외부(호스트PC)로부터 특정 Port (30009) 로 온 요청을 Kind 클러스터 내부에 argocd-server-nodeport라는 NodePort 로 매핑하고, 이 NodePort 는 ArgoCD 리소스로 연결해주는 역할입니다.\nargocd-nodeport.yml\r\nargocd-nodeport 라는 이름의 NodePort 리소스를 정의한 리소스 정의 파일입니다.\n---\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  labels:\r\n    app: argocd-server-nodeport\r\n  name: argocd-server-nodeport\r\n  namespace: argocd\r\nspec:\r\n  ports:\r\n  - name: \"80\"\r\n    port: 80\r\n    targetPort: 8080\r\n    nodePort: 30009\r\n    protocol: TCP\r\n  selector:\r\n    app.kubernetes.io/name: argocd-server\r\n  sessionAffinity: None\r\n  type: NodePort\n이렇게 정의한 nodeport 는 아래와 같이 kubectl 로 클러스터에 리소스를 생성하도록 요청할 수 있습니다.\nkubectl apply -f argocd-nodeport.yml","argocd-생성-스크립트#ArgoCD 생성 스크립트":"여기까지 작성한 모든 ArgoCD 생성 스크립트는 cluster/setup-argocd.sh 파일 내에 정의했습니다.cluster/setup-argocd.sh\necho \"\"\r\necho \"[create] namsepace 'argocd'\"\r\nkubectl create namespace argocd\r\n\r\necho \"\"\r\necho \"[install] kubectl apply -f argoprj/argo-cd/.../install.yaml\"\r\nkubectl -n argocd apply -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\r\n\r\necho \"\"\r\necho \"[configure] --insecure configure\"\r\nkubectl -n argocd patch deployment argocd-server --type json -p='[{\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/0/args\",\"value\":[\"/usr/local/bin/argocd-server\",\"--insecure\"]}]'\r\n\r\necho \"\"\r\necho \"[status] kubectl -n argocd get all\"\r\nkubectl -n argocd get all\r\n\r\necho \"\"\r\necho \"wait(45s) ... \"\r\nsleep 45\r\n\r\necho \"\"\r\necho \"[password!!!] your password\"\r\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\r\n\r\necho \"\"\r\necho \"\"\r\necho \"wait(45s) ... \"\r\nsleep 45\r\n\r\necho \"\"\r\necho \"[setup] nodeport (argocd-nodeport)\"\r\nkubectl apply -f argocd-nodeport.yml\n쉘 스크립트에서 사용하고 있는 argocd-nodeport.yml 파일의 내용은 아래와 같습니다.\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  labels:\r\n    app: argocd-server-nodeport\r\n  name: argocd-server-nodeport\r\n  namespace: argocd\r\nspec:\r\n  ports:\r\n  - name: \"80\"\r\n    port: 80\r\n    targetPort: 8080\r\n    nodePort: 30009\r\n    protocol: TCP\r\n  selector:\r\n    app.kubernetes.io/name: argocd-server\r\n  sessionAffinity: None\r\n  type: NodePort","setupsh#setup.sh":"여기까지 작성한 모든 내용들은 cluster/setup.sh 파일에서 호출하고 있습니다.\r\ncluster/setup.sh 파일의 내용은 아래와 같습니다.cluster/setup.sh\necho \"\"\r\necho \">>> [create-cluster.sh]\"\r\nsource create-cluster.sh\r\n\r\necho \"\"\r\necho \">>> [setup-argocd.sh]\"\r\nsource setup-argocd.sh"}},"/setup/redis-environment":{"title":"Redis Environment","data":{"redis-환경설정#Redis 환경설정":""}},"/setup/reference":{"title":"Reference","data":{"참고했던-스터디-자료들#참고했던 스터디 자료들":"혹시라도 처음으로 쿠버네티스를 스터디해야 하는 분들이거나, 회사에 있는 레거시를 쿠버네티스 기반으로 전환해야 하지만 자료가 없어서 답답하신 분들이 계시다면 아래 자료를 참고해주세요. 저는 혼자 공부하느라 아무것도 모른 채로 몇번 실패해서 읽어본것도 있고  강의를 듣다가 '이건 도저히 안되는 강의네' 하면서 수강을 포기한 강의도 있습니다. 아래에 정리한 자료들은 모두 직접 경험해본 후 실제로 도움이 되었던 자료들입니다.","강의#강의":"한번에 끝내는 CI/CD Docker 부터 GitOps 까지\nKustomize, ArgoCD, EKS 설치 등등 백엔드 개발자가 어렵게 여기는 인프라에 대해 설명이 잘되어 있습니다. 아직 강의를 30% 만 수강해둔 상태여서 더 많은 강의를 들어야 합니다.\n백엔드 개발자를 위한 Kubernetes : 클라우드 네이티브 프로그래밍\n백엔드 개발자 분이 강의를 해주십니다. 강의 내에 해주시는 설명 중에 놓치기 아까운 부분들이 많아서 받아적느라 힘들었습니다. 강의 자료가 PPT에 몇몇 단어나 문장으로만 되어있어서 복습할 때 강의를 한번 더 들어야 한다는 멘탈붕괴에 빠진다는 단점이 있습니다.\n이 강의를 들으신다면, 두 번째 복습때는 꼭 필기를 하셔야 합니다.\n그런데 내용이 정말 알찹니다. 보장합니다.","eks-workshop-studio#EKS Workshop Studio":"Amazon EKS 로 웹 애플리케이션 구축하기\nBuilding Web Applications based on Amazon EKS","블로그#블로그":"blog.naver.com/alice_k106","책#책":"최근 1년 사이에 읽었던 책만 추렸습니다.\n클라우드 네이티브를 위한 쿠버네티스 실전 프로젝트\n핵심만 콕! 쿠버네티스\n쿠버네티스 패턴 - 클라우드 네이티브 애플리케이션 설계와 구현을 위한 24가지 디자인 패턴"}},"/setup/deploy-environment":{"title":"Deploy Environment","data":{"배포환경#배포환경":"","kustomize#Kustomize":"develop, production 사이에서 달라지는 부분들에 대해 kustomize 의 cross-cut, base/overlay 기능을 사용합니다.","argocd-argo-rollouts#ArgoCD, Argo Rollouts":"kustomize 로 구성된 yaml 리소스 정의 파일을 기반으로 일반적인 Rolling Update 방식의 단순배포를 구성하고 배포 과정을 확인해봅니다.\n이 외에도 Blue/Green, Canary 무중단 배포 방식 역시 Argo Rollouts 를 이용해 실습으로 진행해봅니다."}}}